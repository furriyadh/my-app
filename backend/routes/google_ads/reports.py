"""
Google Ads Reports & Analytics
Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ·ÙˆØ± Ù„Ù€ Google Ads

ÙŠÙˆÙØ± Ù†Ø¸Ø§Ù… ØªÙ‚Ø§Ø±ÙŠØ± Ø´Ø§Ù…Ù„ ÙˆÙ…ØªØ·ÙˆØ± Ù„Ù€ Google Ads Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ:
- ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
- ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
- ØªÙ‚Ø§Ø±ÙŠØ± Ù…Ø®ØµØµØ© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®ØµÙŠØµ
- ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØº Ù…ØªØ¹Ø¯Ø¯Ø©
- Ù„ÙˆØ­Ø§Øª Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØ§Ø¹Ù„ÙŠØ©
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
- Ù…Ù‚Ø§Ø±Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±
- ØªÙ‚Ø§Ø±ÙŠØ± ROI ÙˆØ§Ù„Ø¹Ø§Ø¦Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±

Author: Google Ads Analytics Team
Version: 2.2.0
Security Level: Enterprise
Performance: AI-Powered Analytics Engine
"""

import os
import asyncio
import aiohttp
import json
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any, Tuple, Union, Set, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from functools import wraps, lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict, Counter
import hashlib
import uuid
import math
import io
import base64

# Flask imports
from flask import Blueprint, request, jsonify, current_app, send_file, make_response
from flask_jwt_extended import jwt_required, get_jwt_identity, get_jwt

# Third-party imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio

# Local imports
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
logger = logging.getLogger(__name__)

# Ø¥Ù†Ø´Ø§Ø¡ Blueprint Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
google_ads_reports_bp = Blueprint(
    'google_ads_reports',
    __name__,
    url_prefix='/api/google-ads/reports',
    static_folder=None,
    template_folder=None
)

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
REPORTS_SERVICES_STATUS = {
    'google_ads_client': False,
    'database': False,
    'redis': False,
    'validators': False,
    'helpers': False,
    'ai_services': False,
    'data_processing': False,
    'visualization': False
}

try:
    from services.google_ads_client import GoogleAdsClientManager
    REPORTS_SERVICES_STATUS['google_ads_client'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ GoogleAdsClientManager ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.database import DatabaseManager
    REPORTS_SERVICES_STATUS['database'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ DatabaseManager ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.redis_config import cache_set, cache_get, cache_delete
    REPORTS_SERVICES_STATUS['redis'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Redis ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.validators import validate_customer_id, validate_date_range
    REPORTS_SERVICES_STATUS['validators'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Validators ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.helpers import (
        generate_unique_id, sanitize_text, format_currency,
        calculate_performance_score, format_percentage
    )
    REPORTS_SERVICES_STATUS['helpers'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Helpers ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from services.ai_services import DataAnalysisService, PredictionService
    REPORTS_SERVICES_STATUS['ai_services'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ AI Services ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from services.data_processing import DataProcessor, MetricsCalculator
    REPORTS_SERVICES_STATUS['data_processing'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Data Processing ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from services.visualization import ChartGenerator, DashboardBuilder
    REPORTS_SERVICES_STATUS['visualization'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Visualization ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

# ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª
REPORTS_SERVICES_AVAILABLE = any(REPORTS_SERVICES_STATUS.values())
logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø®Ø¯Ù…Ø§Øª Reports - Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {sum(REPORTS_SERVICES_STATUS.values())}/8")

# Ø¥Ø¹Ø¯Ø§Ø¯ Thread Pool Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
reports_executor = ThreadPoolExecutor(max_workers=30, thread_name_prefix="reports_worker")

class ReportType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"""
    PERFORMANCE = "performance"
    CAMPAIGN = "campaign"
    KEYWORD = "keyword"
    AD_GROUP = "ad_group"
    AUDIENCE = "audience"
    GEOGRAPHIC = "geographic"
    DEVICE = "device"
    TIME_BASED = "time_based"
    CONVERSION = "conversion"
    BUDGET = "budget"
    QUALITY_SCORE = "quality_score"
    COMPETITIVE = "competitive"
    CUSTOM = "custom"

class ReportFormat(Enum):
    """ØµÙŠØº Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"""
    JSON = "json"
    CSV = "csv"
    EXCEL = "excel"
    PDF = "pdf"
    HTML = "html"
    CHART = "chart"
    DASHBOARD = "dashboard"

class TimeGranularity(Enum):
    """Ø¯Ù‚Ø© Ø§Ù„ÙˆÙ‚Øª"""
    HOURLY = "hourly"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    YEARLY = "yearly"

class MetricType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
    IMPRESSIONS = "impressions"
    CLICKS = "clicks"
    CTR = "ctr"
    CPC = "cpc"
    COST = "cost"
    CONVERSIONS = "conversions"
    CONVERSION_RATE = "conversion_rate"
    CPA = "cpa"
    ROAS = "roas"
    QUALITY_SCORE = "quality_score"
    IMPRESSION_SHARE = "impression_share"
    SEARCH_IMPRESSION_SHARE = "search_impression_share"

@dataclass
class ReportConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
    report_type: ReportType
    date_range: Dict[str, str]
    metrics: List[MetricType]
    dimensions: List[str] = field(default_factory=list)
    filters: Dict[str, Any] = field(default_factory=dict)
    granularity: TimeGranularity = TimeGranularity.DAILY
    format: ReportFormat = ReportFormat.JSON
    include_charts: bool = True
    include_insights: bool = True
    include_recommendations: bool = True
    custom_segments: List[str] = field(default_factory=list)
    comparison_periods: List[Dict[str, str]] = field(default_factory=list)

@dataclass
class ReportData:
    """Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
    report_id: str
    config: ReportConfig
    data: List[Dict[str, Any]]
    summary: Dict[str, Any]
    insights: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    charts: List[Dict[str, Any]] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    generated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    processing_time: float = 0.0

@dataclass
class DashboardConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
    dashboard_id: str
    title: str
    widgets: List[Dict[str, Any]]
    layout: Dict[str, Any]
    refresh_interval: int = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
    auto_refresh: bool = True
    filters: Dict[str, Any] = field(default_factory=dict)
    permissions: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AnalyticsInsight:
    """Ø±Ø¤ÙŠØ© ØªØ­Ù„ÙŠÙ„ÙŠØ©"""
    insight_id: str
    type: str
    title: str
    description: str
    impact_level: str  # high, medium, low
    confidence_score: float
    supporting_data: Dict[str, Any]
    recommendations: List[str]
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

class DataAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ·ÙˆØ±"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        self.analysis_cache = {}
        self.ml_models = {}
        self.scaler = StandardScaler()
    
    async def analyze_performance_data(self, data: List[Dict[str, Any]], 
                                     config: ReportConfig) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            if not data:
                return {'error': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ­Ù„ÙŠÙ„'}
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ DataFrame
            df = pd.DataFrame(data)
            
            analysis = {
                'basic_stats': await self._calculate_basic_statistics(df),
                'trends': await self._analyze_trends(df, config.granularity),
                'correlations': await self._analyze_correlations(df),
                'anomalies': await self._detect_anomalies(df),
                'segments': await self._analyze_segments(df),
                'predictions': await self._generate_predictions(df),
                'insights': await self._generate_insights(df, config)
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return {'error': str(e)}
    
    async def _calculate_basic_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            stats = {
                'total_rows': len(df),
                'date_range': {
                    'start': df['date'].min() if 'date' in df.columns else None,
                    'end': df['date'].max() if 'date' in df.columns else None
                },
                'metrics_summary': {}
            }
            
            for col in numeric_columns:
                if col in ['impressions', 'clicks', 'cost', 'conversions']:
                    stats['metrics_summary'][col] = {
                        'total': float(df[col].sum()),
                        'average': float(df[col].mean()),
                        'median': float(df[col].median()),
                        'std': float(df[col].std()),
                        'min': float(df[col].min()),
                        'max': float(df[col].max())
                    }
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø´ØªÙ‚Ø©
            if 'impressions' in df.columns and 'clicks' in df.columns:
                total_impressions = df['impressions'].sum()
                total_clicks = df['clicks'].sum()
                stats['derived_metrics'] = {
                    'overall_ctr': (total_clicks / total_impressions * 100) if total_impressions > 0 else 0,
                    'avg_cpc': (df['cost'].sum() / total_clicks) if total_clicks > 0 else 0,
                    'total_conversion_rate': (df['conversions'].sum() / total_clicks * 100) if total_clicks > 0 else 0
                }
            
            return stats
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
            return {}
    
    async def _analyze_trends(self, df: pd.DataFrame, granularity: TimeGranularity) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        try:
            if 'date' not in df.columns:
                return {'error': 'Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ® ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯'}
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            
            trends = {}
            numeric_columns = ['impressions', 'clicks', 'cost', 'conversions', 'ctr', 'cpc']
            
            for col in numeric_columns:
                if col in df.columns:
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ
                    x = np.arange(len(df)).reshape(-1, 1)
                    y = df[col].values
                    
                    model = LinearRegression()
                    model.fit(x, y)
                    
                    slope = model.coef_[0]
                    r_squared = model.score(x, y)
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØºÙŠÙŠØ±
                    if slope > 0.01:
                        direction = 'increasing'
                    elif slope < -0.01:
                        direction = 'decreasing'
                    else:
                        direction = 'stable'
                    
                    # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØºÙŠÙŠØ±
                    if len(df) >= 7:
                        recent_avg = df[col].tail(7).mean()
                        previous_avg = df[col].head(7).mean()
                        change_rate = ((recent_avg - previous_avg) / previous_avg * 100) if previous_avg > 0 else 0
                    else:
                        change_rate = 0
                    
                    trends[col] = {
                        'direction': direction,
                        'slope': float(slope),
                        'r_squared': float(r_squared),
                        'change_rate_percentage': float(change_rate),
                        'trend_strength': 'strong' if abs(slope) > 0.1 else 'moderate' if abs(slope) > 0.05 else 'weak'
                    }
            
            return trends
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª: {e}")
            return {}
    
    async def _analyze_correlations(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª"""
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) < 2:
                return {'error': 'Ø¹Ø¯Ø¯ ØºÙŠØ± ÙƒØ§ÙÙ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©'}
            
            # Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·
            correlation_matrix = df[numeric_columns].corr()
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù‚ÙˆÙ‰ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
            correlations = []
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    col1 = correlation_matrix.columns[i]
                    col2 = correlation_matrix.columns[j]
                    corr_value = correlation_matrix.iloc[i, j]
                    
                    if not np.isnan(corr_value) and abs(corr_value) > 0.3:
                        correlations.append({
                            'metric1': col1,
                            'metric2': col2,
                            'correlation': float(corr_value),
                            'strength': 'strong' if abs(corr_value) > 0.7 else 'moderate' if abs(corr_value) > 0.5 else 'weak',
                            'direction': 'positive' if corr_value > 0 else 'negative'
                        })
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·
            correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)
            
            return {
                'correlation_matrix': correlation_matrix.to_dict(),
                'significant_correlations': correlations[:10],  # Ø£Ù‚ÙˆÙ‰ 10 Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
                'insights': await self._generate_correlation_insights(correlations)
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª: {e}")
            return {}
    
    async def _detect_anomalies(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            anomalies = {}
            numeric_columns = ['impressions', 'clicks', 'cost', 'conversions']
            
            for col in numeric_columns:
                if col in df.columns:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… IQR Ù„ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
                    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                    
                    if len(outliers) > 0:
                        anomalies[col] = {
                            'count': len(outliers),
                            'percentage': (len(outliers) / len(df)) * 100,
                            'outlier_values': outliers[col].tolist(),
                            'bounds': {
                                'lower': float(lower_bound),
                                'upper': float(upper_bound)
                            }
                        }
            
            return anomalies
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°: {e}")
            return {}
    
    async def _analyze_segments(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø±Ø§Ø¦Ø­"""
        try:
            segments = {}
            
            # ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ù‡Ø§Ø² (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
            if 'device' in df.columns:
                device_analysis = df.groupby('device').agg({
                    'impressions': 'sum',
                    'clicks': 'sum',
                    'cost': 'sum',
                    'conversions': 'sum'
                }).reset_index()
                
                device_analysis['ctr'] = (device_analysis['clicks'] / device_analysis['impressions'] * 100)
                device_analysis['conversion_rate'] = (device_analysis['conversions'] / device_analysis['clicks'] * 100)
                
                segments['device'] = device_analysis.to_dict('records')
            
            # ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª (ÙŠÙˆÙ… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹)
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df['day_of_week'] = df['date'].dt.day_name()
                
                day_analysis = df.groupby('day_of_week').agg({
                    'impressions': 'sum',
                    'clicks': 'sum',
                    'cost': 'sum',
                    'conversions': 'sum'
                }).reset_index()
                
                day_analysis['ctr'] = (day_analysis['clicks'] / day_analysis['impressions'] * 100)
                segments['day_of_week'] = day_analysis.to_dict('records')
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ (Ø¹Ø§Ù„ÙŠØŒ Ù…ØªÙˆØ³Ø·ØŒ Ù…Ù†Ø®ÙØ¶)
            if 'ctr' in df.columns:
                df['performance_tier'] = pd.cut(df['ctr'], 
                                              bins=[0, 2, 5, float('inf')], 
                                              labels=['Ù…Ù†Ø®ÙØ¶', 'Ù…ØªÙˆØ³Ø·', 'Ø¹Ø§Ù„ÙŠ'])
                
                performance_analysis = df.groupby('performance_tier').agg({
                    'impressions': 'sum',
                    'clicks': 'sum',
                    'cost': 'sum',
                    'conversions': 'sum'
                }).reset_index()
                
                segments['performance_tier'] = performance_analysis.to_dict('records')
            
            return segments
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø±Ø§Ø¦Ø­: {e}")
            return {}
    
    async def _generate_predictions(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        try:
            if len(df) < 7:  # Ù†Ø­ØªØ§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤
                return {'error': 'Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤'}
            
            predictions = {}
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df = df.sort_values('date')
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø²Ù…Ù†ÙŠØ©
                df['day_of_year'] = df['date'].dt.dayofyear
                df['day_of_week'] = df['date'].dt.dayofweek
                df['month'] = df['date'].dt.month
                
                features = ['day_of_year', 'day_of_week', 'month']
                
                for metric in ['impressions', 'clicks', 'cost', 'conversions']:
                    if metric in df.columns:
                        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        X = df[features].values
                        y = df[metric].values
                        
                        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Random Forest
                        model = RandomForestRegressor(n_estimators=100, random_state=42)
                        model.fit(X, y)
                        
                        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„Ø£ÙŠØ§Ù… Ø§Ù„Ø³Ø¨Ø¹Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©
                        future_dates = pd.date_range(start=df['date'].max() + timedelta(days=1), periods=7)
                        future_features = []
                        
                        for date in future_dates:
                            future_features.append([
                                date.dayofyear,
                                date.dayofweek,
                                date.month
                            ])
                        
                        future_predictions = model.predict(future_features)
                        
                        predictions[metric] = {
                            'next_7_days': [
                                {
                                    'date': date.isoformat(),
                                    'predicted_value': float(pred),
                                    'confidence': 'medium'  # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡Ø§ Ø¨Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
                                }
                                for date, pred in zip(future_dates, future_predictions)
                            ],
                            'total_predicted': float(future_predictions.sum()),
                            'average_predicted': float(future_predictions.mean())
                        }
            
            return predictions
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {e}")
            return {}
    
    async def _generate_insights(self, df: pd.DataFrame, config: ReportConfig) -> List[AnalyticsInsight]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠØ©"""
        insights = []
        
        try:
            # Ø±Ø¤Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù…
            if 'ctr' in df.columns:
                avg_ctr = df['ctr'].mean()
                if avg_ctr < 2.0:
                    insights.append(AnalyticsInsight(
                        insight_id=generate_unique_id('insight') if REPORTS_SERVICES_STATUS['helpers'] else f"insight_{int(time.time())}",
                        type="performance",
                        title="Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± Ù…Ù†Ø®ÙØ¶",
                        description=f"Ù…ØªÙˆØ³Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± {avg_ctr:.2f}% Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (2%)",
                        impact_level="high",
                        confidence_score=0.85,
                        supporting_data={'avg_ctr': avg_ctr, 'benchmark': 2.0},
                        recommendations=[
                            "ØªØ­Ø³ÙŠÙ† Ù†ØµÙˆØµ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª",
                            "Ø¥Ø¶Ø§ÙØ© Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª",
                            "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù"
                        ]
                    ))
                elif avg_ctr > 5.0:
                    insights.append(AnalyticsInsight(
                        insight_id=generate_unique_id('insight') if REPORTS_SERVICES_STATUS['helpers'] else f"insight_{int(time.time())}",
                        type="performance",
                        title="Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± Ù…Ù…ØªØ§Ø²",
                        description=f"Ù…ØªÙˆØ³Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± {avg_ctr:.2f}% Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„Ù…ØªÙˆØ³Ø·",
                        impact_level="medium",
                        confidence_score=0.90,
                        supporting_data={'avg_ctr': avg_ctr, 'benchmark': 2.0},
                        recommendations=[
                            "Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©",
                            "ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø­Ù…Ù„Ø©",
                            "Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©"
                        ]
                    ))
            
            # Ø±Ø¤Ù‰ Ø§Ù„ØªÙƒÙ„ÙØ©
            if 'cost' in df.columns and 'conversions' in df.columns:
                total_cost = df['cost'].sum()
                total_conversions = df['conversions'].sum()
                
                if total_conversions > 0:
                    avg_cpa = total_cost / total_conversions
                    if avg_cpa > 150:
                        insights.append(AnalyticsInsight(
                            insight_id=generate_unique_id('insight') if REPORTS_SERVICES_STATUS['helpers'] else f"insight_{int(time.time())}",
                            type="cost",
                            title="ØªÙƒÙ„ÙØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ø±ØªÙØ¹Ø©",
                            description=f"Ù…ØªÙˆØ³Ø· ØªÙƒÙ„ÙØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„ {avg_cpa:.2f} Ø±ÙŠØ§Ù„ Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨",
                            impact_level="high",
                            confidence_score=0.80,
                            supporting_data={'avg_cpa': avg_cpa, 'total_cost': total_cost, 'total_conversions': total_conversions},
                            recommendations=[
                                "ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©",
                                "ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù‚ØµÙˆØ¯Ø©",
                                "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¹Ø±ÙˆØ¶"
                            ]
                        ))
            
            # Ø±Ø¤Ù‰ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
            if len(df) >= 7 and 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df = df.sort_values('date')
                
                # ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªÙƒÙ„ÙØ©
                if 'cost' in df.columns:
                    recent_cost = df['cost'].tail(3).mean()
                    previous_cost = df['cost'].head(3).mean()
                    
                    if recent_cost > previous_cost * 1.2:
                        insights.append(AnalyticsInsight(
                            insight_id=generate_unique_id('insight') if REPORTS_SERVICES_STATUS['helpers'] else f"insight_{int(time.time())}",
                            type="trend",
                            title="Ø§Ø±ØªÙØ§Ø¹ ÙÙŠ Ø§Ù„ØªÙƒÙ„ÙØ©",
                            description="Ø§Ù„ØªÙƒÙ„ÙØ© ÙÙŠ Ø§Ø±ØªÙØ§Ø¹ Ù…Ø³ØªÙ…Ø± Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©",
                            impact_level="medium",
                            confidence_score=0.75,
                            supporting_data={'recent_cost': recent_cost, 'previous_cost': previous_cost},
                            recommendations=[
                                "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©",
                                "ØªØ­Ø³ÙŠÙ† Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©",
                                "Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù"
                            ]
                        ))
            
            return insights
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¤Ù‰: {e}")
            return []
    
    async def _generate_correlation_insights(self, correlations: List[Dict[str, Any]]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤Ù‰ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·"""
        insights = []
        
        for corr in correlations[:5]:  # Ø£Ù‚ÙˆÙ‰ 5 Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
            if corr['correlation'] > 0.7:
                insights.append(f"Ø§Ø±ØªØ¨Ø§Ø· Ù‚ÙˆÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø¨ÙŠÙ† {corr['metric1']} Ùˆ {corr['metric2']} ({corr['correlation']:.2f})")
            elif corr['correlation'] < -0.7:
                insights.append(f"Ø§Ø±ØªØ¨Ø§Ø· Ù‚ÙˆÙŠ Ø³Ù„Ø¨ÙŠ Ø¨ÙŠÙ† {corr['metric1']} Ùˆ {corr['metric2']} ({corr['correlation']:.2f})")
            elif abs(corr['correlation']) > 0.5:
                direction = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ" if corr['correlation'] > 0 else "Ø³Ù„Ø¨ÙŠ"
                insights.append(f"Ø§Ø±ØªØ¨Ø§Ø· Ù…ØªÙˆØ³Ø· {direction} Ø¨ÙŠÙ† {corr['metric1']} Ùˆ {corr['metric2']} ({corr['correlation']:.2f})")
        
        return insights

class ChartGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©"""
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·
        self.color_palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
        plt.style.use('seaborn-v0_8')
        
    async def generate_performance_charts(self, df: pd.DataFrame, config: ReportConfig) -> List[Dict[str, Any]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø³ÙˆÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        charts = []
        
        try:
            # Ø±Ø³Ù… Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
            if 'date' in df.columns and config.granularity == TimeGranularity.DAILY:
                time_series_chart = await self._create_time_series_chart(df)
                if time_series_chart:
                    charts.append(time_series_chart)
            
            # Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
            metrics_distribution_chart = await self._create_metrics_distribution_chart(df)
            if metrics_distribution_chart:
                charts.append(metrics_distribution_chart)
            
            # Ø±Ø³Ù… Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
            correlation_chart = await self._create_correlation_heatmap(df)
            if correlation_chart:
                charts.append(correlation_chart)
            
            # Ø±Ø³Ù… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø±Ø§Ø¦Ø­
            if 'device' in df.columns:
                device_performance_chart = await self._create_device_performance_chart(df)
                if device_performance_chart:
                    charts.append(device_performance_chart)
            
            return charts
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©: {e}")
            return []
    
    async def _create_time_series_chart(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        try:
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… ØªÙØ§Ø¹Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Plotly
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=('Ø§Ù„Ø¸Ù‡ÙˆØ± ÙˆØ§Ù„Ù†Ù‚Ø±Ø§Øª', 'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±', 'Ø§Ù„ØªÙƒÙ„ÙØ©', 'Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª'),
                specs=[[{"secondary_y": True}, {"secondary_y": False}],
                       [{"secondary_y": False}, {"secondary_y": False}]]
            )
            
            # Ø§Ù„Ø¸Ù‡ÙˆØ± ÙˆØ§Ù„Ù†Ù‚Ø±Ø§Øª
            if 'impressions' in df.columns:
                fig.add_trace(
                    go.Scatter(x=df['date'], y=df['impressions'], name='Ø§Ù„Ø¸Ù‡ÙˆØ±', line=dict(color='blue')),
                    row=1, col=1
                )
            
            if 'clicks' in df.columns:
                fig.add_trace(
                    go.Scatter(x=df['date'], y=df['clicks'], name='Ø§Ù„Ù†Ù‚Ø±Ø§Øª', line=dict(color='orange')),
                    row=1, col=1, secondary_y=True
                )
            
            # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±
            if 'ctr' in df.columns:
                fig.add_trace(
                    go.Scatter(x=df['date'], y=df['ctr'], name='Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± %', line=dict(color='green')),
                    row=1, col=2
                )
            
            # Ø§Ù„ØªÙƒÙ„ÙØ©
            if 'cost' in df.columns:
                fig.add_trace(
                    go.Scatter(x=df['date'], y=df['cost'], name='Ø§Ù„ØªÙƒÙ„ÙØ©', line=dict(color='red')),
                    row=2, col=1
                )
            
            # Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª
            if 'conversions' in df.columns:
                fig.add_trace(
                    go.Scatter(x=df['date'], y=df['conversions'], name='Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª', line=dict(color='purple')),
                    row=2, col=2
                )
            
            fig.update_layout(
                title='Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø²Ù…Ù†ÙŠØ©',
                height=600,
                showlegend=True
            )
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ JSON
            chart_json = pio.to_json(fig)
            
            return {
                'type': 'time_series',
                'title': 'Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø²Ù…Ù†ÙŠØ©',
                'data': json.loads(chart_json),
                'format': 'plotly'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©: {e}")
            return None
    
    async def _create_metrics_distribution_chart(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        try:
            metrics = ['impressions', 'clicks', 'cost', 'conversions']
            available_metrics = [m for m in metrics if m in df.columns]
            
            if not available_metrics:
                return None
            
            fig = go.Figure()
            
            for metric in available_metrics:
                fig.add_trace(go.Box(
                    y=df[metric],
                    name=metric,
                    boxpoints='outliers'
                ))
            
            fig.update_layout(
                title='ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³',
                yaxis_title='Ø§Ù„Ù‚ÙŠÙ…',
                xaxis_title='Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³'
            )
            
            chart_json = pio.to_json(fig)
            
            return {
                'type': 'box_plot',
                'title': 'ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³',
                'data': json.loads(chart_json),
                'format': 'plotly'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³: {e}")
            return None
    
    async def _create_correlation_heatmap(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª"""
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) < 2:
                return None
            
            correlation_matrix = df[numeric_columns].corr()
            
            fig = go.Figure(data=go.Heatmap(
                z=correlation_matrix.values,
                x=correlation_matrix.columns,
                y=correlation_matrix.columns,
                colorscale='RdBu',
                zmid=0,
                text=correlation_matrix.round(2).values,
                texttemplate="%{text}",
                textfont={"size": 10}
            ))
            
            fig.update_layout(
                title='Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³',
                xaxis_title='Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³',
                yaxis_title='Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³'
            )
            
            chart_json = pio.to_json(fig)
            
            return {
                'type': 'heatmap',
                'title': 'Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·',
                'data': json.loads(chart_json),
                'format': 'plotly'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·: {e}")
            return None
    
    async def _create_device_performance_chart(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©"""
        try:
            device_data = df.groupby('device').agg({
                'impressions': 'sum',
                'clicks': 'sum',
                'cost': 'sum',
                'conversions': 'sum'
            }).reset_index()
            
            device_data['ctr'] = (device_data['clicks'] / device_data['impressions'] * 100)
            
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=('Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ù‡Ø§Ø²', 'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± Ø­Ø³Ø¨ Ø§Ù„Ø¬Ù‡Ø§Ø²'),
                specs=[[{"type": "pie"}, {"type": "bar"}]]
            )
            
            # Ø±Ø³Ù… Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„ØªÙˆØ²ÙŠØ¹
            fig.add_trace(
                go.Pie(labels=device_data['device'], values=device_data['clicks'], name="Ø§Ù„Ù†Ù‚Ø±Ø§Øª"),
                row=1, col=1
            )
            
            # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±
            fig.add_trace(
                go.Bar(x=device_data['device'], y=device_data['ctr'], name="Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± %"),
                row=1, col=2
            )
            
            fig.update_layout(title='Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©')
            
            chart_json = pio.to_json(fig)
            
            return {
                'type': 'device_performance',
                'title': 'Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©',
                'data': json.loads(chart_json),
                'format': 'plotly'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©: {e}")
            return None

class ReportGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ØªØ·ÙˆØ±"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"""
        self.google_ads_client = GoogleAdsClientManager() if REPORTS_SERVICES_STATUS['google_ads_client'] else None
        self.db_manager = DatabaseManager() if REPORTS_SERVICES_STATUS['database'] else None
        self.data_analyzer = DataAnalyzer()
        self.chart_generator = ChartGenerator()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®Ø¯Ù…Ø©
        self.service_stats = {
            'total_reports_generated': 0,
            'reports_by_type': defaultdict(int),
            'average_processing_time': 0.0,
            'last_report_generated': None,
            'cache_hit_rate': 0.0
        }
        
        logger.info("ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ØªØ·ÙˆØ±")
    
    async def generate_report(self, customer_id: str, config: ReportConfig) -> ReportData:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ±"""
        start_time = time.time()
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            report_id = generate_unique_id('report') if REPORTS_SERVICES_STATUS['helpers'] else f"report_{int(time.time())}"
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            cache_key = f"report_{customer_id}_{hash(str(asdict(config)))}"
            cached_report = await self._get_cached_report(cache_key)
            
            if cached_report:
                logger.info(f"ØªÙ… Ø¬Ù„Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {report_id}")
                self.service_stats['cache_hit_rate'] += 1
                return cached_report
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            raw_data = await self._fetch_report_data(customer_id, config)
            
            if not raw_data:
                return ReportData(
                    report_id=report_id,
                    config=config,
                    data=[],
                    summary={'error': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ø©'},
                    processing_time=time.time() - start_time
                )
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            analysis = await self.data_analyzer.analyze_performance_data(raw_data, config)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
            charts = []
            if config.include_charts:
                df = pd.DataFrame(raw_data)
                charts = await self.chart_generator.generate_performance_charts(df, config)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù„Ø®Øµ
            summary = await self._generate_summary(raw_data, analysis)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¤Ù‰
            insights = []
            if config.include_insights and 'insights' in analysis:
                insights = [insight.description for insight in analysis['insights']]
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = []
            if config.include_recommendations:
                recommendations = await self._generate_recommendations(raw_data, analysis)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            report = ReportData(
                report_id=report_id,
                config=config,
                data=raw_data,
                summary=summary,
                insights=insights,
                recommendations=recommendations,
                charts=charts,
                metadata={
                    'total_rows': len(raw_data),
                    'analysis_results': analysis,
                    'generation_timestamp': datetime.now(timezone.utc).isoformat()
                },
                processing_time=time.time() - start_time
            )
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            await self._cache_report(cache_key, report)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.service_stats['total_reports_generated'] += 1
            self.service_stats['reports_by_type'][config.report_type.value] += 1
            self.service_stats['last_report_generated'] = datetime.now(timezone.utc)
            
            # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            current_avg = self.service_stats['average_processing_time']
            total_reports = self.service_stats['total_reports_generated']
            self.service_stats['average_processing_time'] = (
                (current_avg * (total_reports - 1) + report.processing_time) / total_reports
            )
            
            return report
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
            return ReportData(
                report_id=f"error_{int(time.time())}",
                config=config,
                data=[],
                summary={'error': str(e)},
                processing_time=time.time() - start_time
            )
    
    async def export_report(self, report: ReportData, format: ReportFormat) -> Dict[str, Any]:
        """ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨ØµÙŠØºØ© Ù…Ø­Ø¯Ø¯Ø©"""
        try:
            if format == ReportFormat.JSON:
                return await self._export_json(report)
            elif format == ReportFormat.CSV:
                return await self._export_csv(report)
            elif format == ReportFormat.EXCEL:
                return await self._export_excel(report)
            elif format == ReportFormat.PDF:
                return await self._export_pdf(report)
            elif format == ReportFormat.HTML:
                return await self._export_html(report)
            else:
                return {'error': f'ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {format.value}'}
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
            return {'error': str(e)}
    
    async def create_dashboard(self, customer_id: str, config: DashboardConfig) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
        try:
            dashboard_data = {
                'dashboard_id': config.dashboard_id,
                'title': config.title,
                'widgets': [],
                'layout': config.layout,
                'last_updated': datetime.now(timezone.utc).isoformat()
            }
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ widget
            for widget_config in config.widgets:
                widget_data = await self._generate_widget_data(customer_id, widget_config)
                dashboard_data['widgets'].append(widget_data)
            
            return {
                'success': True,
                'dashboard': dashboard_data
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: {e}")
            return {'success': False, 'error': str(e)}
    
    # Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
    async def _fetch_report_data(self, customer_id: str, config: ReportConfig) -> List[Dict[str, Any]]:
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Ads API
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ³ØªØ®Ø¯Ù… Google Ads API
        
        sample_data = []
        start_date = datetime.fromisoformat(config.date_range['start_date'])
        end_date = datetime.fromisoformat(config.date_range['end_date'])
        
        current_date = start_date
        while current_date <= end_date:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ…ÙŠØ©
            daily_data = {
                'date': current_date.isoformat(),
                'impressions': np.random.randint(1000, 10000),
                'clicks': np.random.randint(50, 500),
                'cost': np.random.uniform(100, 1000),
                'conversions': np.random.randint(1, 25),
                'device': np.random.choice(['desktop', 'mobile', 'tablet']),
                'campaign_id': f"campaign_{np.random.randint(1, 5)}"
            }
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø´ØªÙ‚Ø©
            daily_data['ctr'] = (daily_data['clicks'] / daily_data['impressions']) * 100
            daily_data['cpc'] = daily_data['cost'] / daily_data['clicks'] if daily_data['clicks'] > 0 else 0
            daily_data['conversion_rate'] = (daily_data['conversions'] / daily_data['clicks']) * 100 if daily_data['clicks'] > 0 else 0
            daily_data['cpa'] = daily_data['cost'] / daily_data['conversions'] if daily_data['conversions'] > 0 else 0
            
            sample_data.append(daily_data)
            current_date += timedelta(days=1)
        
        return sample_data
    
    async def _generate_summary(self, data: List[Dict[str, Any]], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        if not data:
            return {'error': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª'}
        
        df = pd.DataFrame(data)
        
        summary = {
            'period': {
                'start_date': df['date'].min() if 'date' in df.columns else None,
                'end_date': df['date'].max() if 'date' in df.columns else None,
                'total_days': len(df)
            },
            'totals': {
                'impressions': int(df['impressions'].sum()) if 'impressions' in df.columns else 0,
                'clicks': int(df['clicks'].sum()) if 'clicks' in df.columns else 0,
                'cost': float(df['cost'].sum()) if 'cost' in df.columns else 0,
                'conversions': int(df['conversions'].sum()) if 'conversions' in df.columns else 0
            },
            'averages': {
                'daily_impressions': float(df['impressions'].mean()) if 'impressions' in df.columns else 0,
                'daily_clicks': float(df['clicks'].mean()) if 'clicks' in df.columns else 0,
                'daily_cost': float(df['cost'].mean()) if 'cost' in df.columns else 0,
                'daily_conversions': float(df['conversions'].mean()) if 'conversions' in df.columns else 0
            },
            'performance_metrics': {}
        }
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        if summary['totals']['impressions'] > 0:
            summary['performance_metrics']['overall_ctr'] = (summary['totals']['clicks'] / summary['totals']['impressions']) * 100
        
        if summary['totals']['clicks'] > 0:
            summary['performance_metrics']['overall_cpc'] = summary['totals']['cost'] / summary['totals']['clicks']
            summary['performance_metrics']['overall_conversion_rate'] = (summary['totals']['conversions'] / summary['totals']['clicks']) * 100
        
        if summary['totals']['conversions'] > 0:
            summary['performance_metrics']['overall_cpa'] = summary['totals']['cost'] / summary['totals']['conversions']
        
        # Ø¥Ø¶Ø§ÙØ© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        if 'basic_stats' in analysis:
            summary['analysis_summary'] = analysis['basic_stats']
        
        return summary
    
    async def _generate_recommendations(self, data: List[Dict[str, Any]], analysis: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        recommendations = []
        
        if not data:
            return recommendations
        
        df = pd.DataFrame(data)
        
        # ØªÙˆØµÙŠØ§Øª Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±
        if 'ctr' in df.columns:
            avg_ctr = df['ctr'].mean()
            if avg_ctr < 2.0:
                recommendations.append("ØªØ­Ø³ÙŠÙ† Ù†ØµÙˆØµ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±")
                recommendations.append("Ø¥Ø¶Ø§ÙØ© Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¸Ù‡ÙˆØ±")
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªÙƒÙ„ÙØ©
        if 'cost' in df.columns and 'conversions' in df.columns:
            total_cost = df['cost'].sum()
            total_conversions = df['conversions'].sum()
            
            if total_conversions > 0:
                avg_cpa = total_cost / total_conversions
                if avg_cpa > 100:
                    recommendations.append("ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ ØªÙƒÙ„ÙØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„")
                    recommendations.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù‚ØµÙˆØ¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„")
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
        if 'trends' in analysis:
            trends = analysis['trends']
            for metric, trend_data in trends.items():
                if trend_data.get('direction') == 'decreasing' and metric in ['clicks', 'conversions']:
                    recommendations.append(f"Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨ØªØ­Ø³ÙŠÙ† {metric} Ø­ÙŠØ« ÙŠØ¸Ù‡Ø± Ø§ØªØ¬Ø§Ù‡ ØªÙ†Ø§Ø²Ù„ÙŠ")
        
        return recommendations
    
    async def _get_cached_report(self, cache_key: str) -> Optional[ReportData]:
        """Ø¬Ù„Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        if REPORTS_SERVICES_STATUS['redis']:
            try:
                cached_data = cache_get(cache_key)
                if cached_data:
                    return ReportData(**json.loads(cached_data))
            except Exception as e:
                logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
        
        return None
    
    async def _cache_report(self, cache_key: str, report: ReportData) -> None:
        """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        if REPORTS_SERVICES_STATUS['redis']:
            try:
                cache_set(cache_key, json.dumps(asdict(report), default=str), expire=3600)  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
            except Exception as e:
                logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
    
    def get_service_stats(self) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®Ø¯Ù…Ø©"""
        return {
            **self.service_stats,
            'services_status': REPORTS_SERVICES_STATUS,
            'last_updated': datetime.now(timezone.utc).isoformat()
        }

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
report_generator = ReportGenerator()

# ===========================================
# API Routes - Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©
# ===========================================

@google_ads_reports_bp.route('/generate', methods=['POST'])
@jwt_required()
async def generate_report():
    """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ±"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        config = ReportConfig(
            report_type=ReportType(data.get('report_type', 'performance')),
            date_range=data.get('date_range', {
                'start_date': (datetime.now() - timedelta(days=30)).isoformat(),
                'end_date': datetime.now().isoformat()
            }),
            metrics=[MetricType(m) for m in data.get('metrics', ['impressions', 'clicks', 'cost'])],
            dimensions=data.get('dimensions', []),
            filters=data.get('filters', {}),
            granularity=TimeGranularity(data.get('granularity', 'daily')),
            format=ReportFormat(data.get('format', 'json')),
            include_charts=data.get('include_charts', True),
            include_insights=data.get('include_insights', True),
            include_recommendations=data.get('include_recommendations', True)
        )
        
        customer_id = data.get('customer_id', '')
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = await report_generator.generate_report(customer_id, config)
        
        return jsonify({
            'success': True,
            'report': asdict(report)
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±',
            'message': str(e)
        }), 500

@google_ads_reports_bp.route('/<report_id>/export', methods=['POST'])
@jwt_required()
async def export_report(report_id: str):
    """ØªØµØ¯ÙŠØ± ØªÙ‚Ø±ÙŠØ±"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        format = ReportFormat(data.get('format', 'json'))
        
        # Ø¬Ù„Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± (Ù…Ø­Ø§ÙƒØ§Ø©)
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ¬Ù„Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        sample_report = ReportData(
            report_id=report_id,
            config=ReportConfig(
                report_type=ReportType.PERFORMANCE,
                date_range={'start_date': '2024-01-01', 'end_date': '2024-01-31'},
                metrics=[MetricType.IMPRESSIONS, MetricType.CLICKS]
            ),
            data=[],
            summary={}
        )
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        export_result = await report_generator.export_report(sample_report, format)
        
        return jsonify({
            'success': True,
            'export_result': export_result
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±',
            'message': str(e)
        }), 500

@google_ads_reports_bp.route('/dashboard', methods=['POST'])
@jwt_required()
async def create_dashboard():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        config = DashboardConfig(
            dashboard_id=data.get('dashboard_id', generate_unique_id('dashboard') if REPORTS_SERVICES_STATUS['helpers'] else f"dashboard_{int(time.time())}"),
            title=data.get('title', 'Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Google Ads'),
            widgets=data.get('widgets', []),
            layout=data.get('layout', {}),
            refresh_interval=data.get('refresh_interval', 300),
            auto_refresh=data.get('auto_refresh', True)
        )
        
        customer_id = data.get('customer_id', '')
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        result = await report_generator.create_dashboard(customer_id, config)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª',
            'message': str(e)
        }), 500

@google_ads_reports_bp.route('/stats', methods=['GET'])
@jwt_required()
def get_reports_stats():
    """Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"""
    try:
        stats = report_generator.get_service_stats()
        
        return jsonify({
            'success': True,
            'stats': stats,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±',
            'message': str(e)
        }), 500

@google_ads_reports_bp.route('/health', methods=['GET'])
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø®Ø¯Ù…Ø© Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"""
    try:
        health_status = {
            'service': 'Google Ads Reports',
            'status': 'healthy',
            'version': '2.2.0',
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'services_status': REPORTS_SERVICES_STATUS,
            'total_reports_generated': report_generator.service_stats['total_reports_generated']
        }
        
        # ÙØ­Øµ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if not any(REPORTS_SERVICES_STATUS.values()):
            health_status['status'] = 'degraded'
            health_status['warning'] = 'Ø¨Ø¹Ø¶ Ø§Ù„Ø®Ø¯Ù…Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­Ø©'
        
        return jsonify(health_status)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØµØ­Ø©: {e}")
        return jsonify({
            'service': 'Google Ads Reports',
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }), 500

# ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Blueprint
logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Google Ads Reports Blueprint - Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ù…ØªØ§Ø­Ø©: {REPORTS_SERVICES_AVAILABLE}")
logger.info(f"ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª: {sum(REPORTS_SERVICES_STATUS.values())}/8 Ù…ØªØ§Ø­Ø©")

# ØªØµØ¯ÙŠØ± Blueprint ÙˆØ§Ù„ÙƒÙ„Ø§Ø³Ø§Øª
__all__ = [
    'google_ads_reports_bp',
    'ReportGenerator',
    'ReportConfig',
    'ReportData',
    'DashboardConfig',
    'AnalyticsInsight',
    'DataAnalyzer',
    'ChartGenerator',
    'ReportType',
    'ReportFormat',
    'TimeGranularity',
    'MetricType'
]

