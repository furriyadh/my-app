"""
Google Ads Discovery Service
Ø®Ø¯Ù…Ø© Ø§ÙƒØªØ´Ø§Ù Google Ads Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ§Ù„Ù…ØªØ·ÙˆØ±Ø©

ÙŠÙˆÙØ± ÙˆØ¸Ø§Ø¦Ù Ø§ÙƒØªØ´Ø§Ù ÙˆØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„Ø© Ù„Ø­Ø³Ø§Ø¨Ø§Øª Google Ads Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ:
- Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª ÙˆØ§Ù„Ø­Ù…Ù„Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
- Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ† ÙˆØ§Ù„Ø³ÙˆÙ‚
- ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ§Ù„ØªÙ†Ø¨Ø¤Ø§Øª

Author: Google Ads AI Platform Team
Version: 2.1.0
Security Level: Enterprise
Performance: Optimized with AI
"""

import os
import asyncio
import aiohttp
import json
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any, Tuple, Union, Set
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from functools import wraps, lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict, Counter
import hashlib
import uuid

# Flask imports
from flask import Blueprint, request, jsonify, current_app
from flask_jwt_extended import jwt_required, get_jwt_identity, get_jwt

# Third-party imports
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Local imports
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
logger = logging.getLogger(__name__)

# Ø¥Ù†Ø´Ø§Ø¡ Blueprint Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
google_ads_discovery_bp = Blueprint(
    'google_ads_discovery',
    __name__,
    url_prefix='/api/google-ads/discovery',
    static_folder=None,
    template_folder=None
)

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
SERVICES_STATUS = {
    'google_ads_client': False,
    'oauth_manager': False,
    'validators': False,
    'helpers': False,
    'database': False,
    'redis': False,
    'ai_services': False
}

try:
    from services.google_ads_client import GoogleAdsClient
    SERVICES_STATUS['google_ads_client'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ GoogleAdsClient ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from routes.google_ads.oauth import oauth_manager
    SERVICES_STATUS['oauth_manager'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ OAuth Manager ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.validators import validate_customer_id, validate_discovery_params
    SERVICES_STATUS['validators'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Validators ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.helpers import (
        generate_unique_id, sanitize_text, format_currency,
        calculate_performance_score, extract_keywords_from_text
    )
    SERVICES_STATUS['helpers'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Helpers ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.database import DatabaseManager
    SERVICES_STATUS['database'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ DatabaseManager ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.redis_config import cache_set, cache_get, cache_delete
    SERVICES_STATUS['redis'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Redis ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from services.ai_services import AIAnalysisService, KeywordAnalyzer, CompetitorAnalyzer
    SERVICES_STATUS['ai_services'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ AI Services ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

# ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª
DISCOVERY_SERVICES_AVAILABLE = any(SERVICES_STATUS.values())
logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø®Ø¯Ù…Ø§Øª Discovery - Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {sum(SERVICES_STATUS.values())}/7")

# Ø¥Ø¹Ø¯Ø§Ø¯ Thread Pool Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
executor = ThreadPoolExecutor(max_workers=25, thread_name_prefix="discovery_worker")

class DiscoveryType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù"""
    ACCOUNTS = auto()
    CAMPAIGNS = auto()
    AD_GROUPS = auto()
    KEYWORDS = auto()
    COMPETITORS = auto()
    OPPORTUNITIES = auto()
    TRENDS = auto()
    PERFORMANCE = auto()

class AnalysisDepth(Enum):
    """Ø¹Ù…Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
    BASIC = "basic"
    STANDARD = "standard"
    ADVANCED = "advanced"
    COMPREHENSIVE = "comprehensive"

class OpportunityType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙØ±Øµ"""
    KEYWORD_EXPANSION = "keyword_expansion"
    BID_OPTIMIZATION = "bid_optimization"
    AD_COPY_IMPROVEMENT = "ad_copy_improvement"
    AUDIENCE_TARGETING = "audience_targeting"
    BUDGET_REALLOCATION = "budget_reallocation"
    NEGATIVE_KEYWORDS = "negative_keywords"
    LANDING_PAGE_OPTIMIZATION = "landing_page_optimization"

@dataclass
class DiscoveryConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙƒØªØ´Ø§Ù"""
    customer_id: str
    discovery_types: List[DiscoveryType] = field(default_factory=lambda: [DiscoveryType.ACCOUNTS])
    analysis_depth: AnalysisDepth = AnalysisDepth.STANDARD
    include_historical_data: bool = True
    historical_days: int = 90
    include_competitor_analysis: bool = False
    include_ai_insights: bool = True
    max_results_per_type: int = 100
    enable_caching: bool = True
    cache_duration_hours: int = 24
    parallel_processing: bool = True
    include_performance_metrics: bool = True
    language: str = "ar"
    currency: str = "SAR"
    timezone: str = "Asia/Riyadh"

@dataclass
class AccountInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨"""
    customer_id: str
    name: str
    currency_code: str
    time_zone: str
    status: str
    account_type: str
    manager_customer_id: Optional[str] = None
    descriptive_name: Optional[str] = None
    can_manage_clients: bool = False
    test_account: bool = False
    auto_tagging_enabled: bool = False
    tracking_url_template: Optional[str] = None
    final_url_suffix: Optional[str] = None
    created_date: Optional[datetime] = None
    last_modified_time: Optional[datetime] = None

@dataclass
class CampaignInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­Ù…Ù„Ø©"""
    campaign_id: str
    name: str
    status: str
    campaign_type: str
    serving_status: str
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    budget_amount: Optional[float] = None
    budget_type: Optional[str] = None
    bidding_strategy: Optional[str] = None
    target_locations: List[str] = field(default_factory=list)
    target_languages: List[str] = field(default_factory=list)
    ad_groups_count: int = 0
    keywords_count: int = 0
    ads_count: int = 0
    performance_metrics: Dict[str, Any] = field(default_factory=dict)

@dataclass
class KeywordInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
    keyword_id: str
    text: str
    match_type: str
    status: str
    bid_amount: Optional[float] = None
    quality_score: Optional[int] = None
    search_volume: Optional[int] = None
    competition: Optional[str] = None
    suggested_bid: Optional[float] = None
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    ai_insights: Dict[str, Any] = field(default_factory=dict)

@dataclass
class OpportunityInsight:
    """ÙØ±ØµØ© Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
    opportunity_id: str
    type: OpportunityType
    title: str
    description: str
    impact_score: float
    effort_score: float
    priority: str
    estimated_impact: Dict[str, Any]
    recommended_actions: List[str]
    supporting_data: Dict[str, Any]
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

@dataclass
class CompetitorInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†Ø§ÙØ³"""
    competitor_id: str
    domain: str
    name: Optional[str] = None
    estimated_budget: Optional[float] = None
    ad_count: Optional[int] = None
    keyword_overlap: Optional[float] = None
    position_overlap: Optional[float] = None
    shared_keywords: List[str] = field(default_factory=list)
    competitive_metrics: Dict[str, Any] = field(default_factory=dict)

class PerformanceAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªØ·ÙˆØ±"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        self.metrics_cache = {}
        self.benchmark_data = {}
    
    def calculate_performance_score(self, metrics: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            # ÙˆØ²Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
            weights = {
                'ctr': 0.25,  # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±
                'conversion_rate': 0.30,  # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„
                'cost_per_conversion': 0.20,  # ØªÙƒÙ„ÙØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„
                'quality_score': 0.15,  # Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©
                'impression_share': 0.10  # Ø­ØµØ© Ø§Ù„Ø¸Ù‡ÙˆØ±
            }
            
            score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in metrics and metrics[metric] is not None:
                    normalized_value = self._normalize_metric(metric, metrics[metric])
                    score += normalized_value * weight
                    total_weight += weight
            
            return (score / total_weight * 100) if total_weight > 0 else 0.0
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return 0.0
    
    def _normalize_metric(self, metric_name: str, value: float) -> float:
        """ØªØ·Ø¨ÙŠØ¹ Ù‚ÙŠÙ… Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ©
        normalization_rules = {
            'ctr': lambda x: min(x / 5.0, 1.0),  # 5% CTR = 100%
            'conversion_rate': lambda x: min(x / 10.0, 1.0),  # 10% CR = 100%
            'cost_per_conversion': lambda x: max(1.0 - (x / 1000.0), 0.0),  # Ø£Ù‚Ù„ ØªÙƒÙ„ÙØ© = Ø£ÙØ¶Ù„
            'quality_score': lambda x: x / 10.0,  # Ù…Ù† 10
            'impression_share': lambda x: x / 100.0  # Ù…Ù† 100%
        }
        
        if metric_name in normalization_rules:
            return normalization_rules[metric_name](value)
        
        return min(value / 100.0, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ø§ÙØªØ±Ø§Ø¶ÙŠ

class AIInsightsEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¤Ù‰ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.keyword_clusters = {}
        self.trend_patterns = {}
    
    async def analyze_keywords(self, keywords: List[KeywordInfo]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            if not keywords:
                return {'clusters': [], 'insights': [], 'recommendations': []}
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
            keyword_texts = [kw.text for kw in keywords]
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            clusters = await self._cluster_keywords(keyword_texts)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
            performance_insights = await self._analyze_keyword_performance(keywords)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = await self._generate_keyword_recommendations(keywords, clusters)
            
            return {
                'clusters': clusters,
                'performance_insights': performance_insights,
                'recommendations': recommendations,
                'total_keywords': len(keywords),
                'analysis_timestamp': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {e}")
            return {'error': str(e)}
    
    async def _cluster_keywords(self, keyword_texts: List[str]) -> List[Dict[str, Any]]:
        """ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        try:
            if len(keyword_texts) < 3:
                return [{'cluster_id': 0, 'keywords': keyword_texts, 'theme': 'Ø¹Ø§Ù…'}]
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ vectors
            tfidf_matrix = self.vectorizer.fit_transform(keyword_texts)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
            n_clusters = min(5, len(keyword_texts) // 3)
            
            # ØªØ·Ø¨ÙŠÙ‚ K-means
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            clusters = []
            for i in range(n_clusters):
                cluster_keywords = [keyword_texts[j] for j, label in enumerate(cluster_labels) if label == i]
                theme = await self._extract_cluster_theme(cluster_keywords)
                
                clusters.append({
                    'cluster_id': i,
                    'keywords': cluster_keywords,
                    'theme': theme,
                    'size': len(cluster_keywords)
                })
            
            return clusters
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {e}")
            return []
    
    async def _extract_cluster_theme(self, keywords: List[str]) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
            word_freq = Counter()
            for keyword in keywords:
                words = keyword.lower().split()
                word_freq.update(words)
            
            # Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹
            most_common = word_freq.most_common(3)
            if most_common:
                return ' + '.join([word for word, _ in most_common])
            
            return 'Ø¹Ø§Ù…'
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {e}")
            return 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
    
    async def _analyze_keyword_performance(self, keywords: List[KeywordInfo]) -> List[Dict[str, Any]]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        insights = []
        
        try:
            # ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©
            quality_scores = [kw.quality_score for kw in keywords if kw.quality_score]
            if quality_scores:
                avg_quality = np.mean(quality_scores)
                insights.append({
                    'type': 'quality_analysis',
                    'message': f'Ù…ØªÙˆØ³Ø· Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©: {avg_quality:.1f}',
                    'recommendation': 'ØªØ­Ø³ÙŠÙ† Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©' if avg_quality < 7 else 'Ù†Ù‚Ø§Ø· Ø¬ÙˆØ¯Ø© Ù…Ù…ØªØ§Ø²Ø©'
                })
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            match_types = Counter([kw.match_type for kw in keywords])
            insights.append({
                'type': 'match_type_distribution',
                'data': dict(match_types),
                'recommendation': 'ØªÙˆØ§Ø²Ù† Ø¬ÙŠØ¯ ÙÙŠ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©' if len(match_types) > 1 else 'ØªÙ†ÙˆÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©'
            })
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø©
            statuses = Counter([kw.status for kw in keywords])
            active_ratio = statuses.get('ENABLED', 0) / len(keywords) * 100
            insights.append({
                'type': 'status_analysis',
                'active_percentage': active_ratio,
                'recommendation': 'Ù†Ø³Ø¨Ø© ØªÙØ¹ÙŠÙ„ Ø¬ÙŠØ¯Ø©' if active_ratio > 80 else 'ØªØ­Ø³ÙŠÙ† Ù†Ø³Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©'
            })
            
            return insights
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {e}")
            return []
    
    async def _generate_keyword_recommendations(self, keywords: List[KeywordInfo], clusters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        recommendations = []
        
        try:
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©
            low_quality_keywords = [kw for kw in keywords if kw.quality_score and kw.quality_score < 5]
            if low_quality_keywords:
                recommendations.append({
                    'type': 'quality_improvement',
                    'priority': 'high',
                    'title': 'ØªØ­Ø³ÙŠÙ† Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©',
                    'description': f'ÙŠÙˆØ¬Ø¯ {len(low_quality_keywords)} ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ù†Ù‚Ø§Ø· Ø¬ÙˆØ¯Ø© Ù…Ù†Ø®ÙØ¶Ø©',
                    'action': 'Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù‚ØµÙˆØ¯Ø©'
                })
            
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
            for cluster in clusters:
                if cluster['size'] > 10:
                    recommendations.append({
                        'type': 'keyword_expansion',
                        'priority': 'medium',
                        'title': f'ØªÙˆØ³ÙŠØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø© {cluster["theme"]}',
                        'description': f'Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø© ({cluster["size"]} ÙƒÙ„Ù…Ø©) ÙŠÙ…ÙƒÙ† ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§',
                        'action': 'Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¥Ø¹Ù„Ø§Ù†ÙŠØ© Ù…Ù†ÙØµÙ„Ø©'
                    })
            
            # ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ù…Ø©
            if len(keywords) < 20:
                recommendations.append({
                    'type': 'keyword_expansion',
                    'priority': 'medium',
                    'title': 'ØªÙˆØ³ÙŠØ¹ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©',
                    'description': 'Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù‚Ù„ÙŠÙ„ Ù†Ø³Ø¨ÙŠØ§Ù‹',
                    'action': 'Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø°Ø§Øª ØµÙ„Ø©'
                })
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {e}")
            return []

class OpportunityDetector:
    """ÙƒØ§Ø´Ù Ø§Ù„ÙØ±Øµ Ø§Ù„Ù…ØªØ·ÙˆØ±"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´Ù Ø§Ù„ÙØ±Øµ"""
        self.opportunity_rules = self._load_opportunity_rules()
        self.performance_analyzer = PerformanceAnalyzer()
    
    def _load_opportunity_rules(self) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ"""
        return {
            'low_ctr_threshold': 2.0,  # Ø£Ù‚Ù„ Ù…Ù† 2% CTR
            'high_cpc_threshold': 10.0,  # Ø£Ø¹Ù„Ù‰ Ù…Ù† 10 Ø±ÙŠØ§Ù„ CPC
            'low_quality_score_threshold': 5,  # Ø£Ù‚Ù„ Ù…Ù† 5 Ù†Ù‚Ø§Ø· Ø¬ÙˆØ¯Ø©
            'low_impression_share_threshold': 50.0,  # Ø£Ù‚Ù„ Ù…Ù† 50% Ø­ØµØ© Ø¸Ù‡ÙˆØ±
            'high_cost_per_conversion_threshold': 100.0  # Ø£Ø¹Ù„Ù‰ Ù…Ù† 100 Ø±ÙŠØ§Ù„ Ù„ÙƒÙ„ ØªØ­ÙˆÙŠÙ„
        }
    
    async def detect_opportunities(self, campaigns: List[CampaignInfo], keywords: List[KeywordInfo]) -> List[OpportunityInsight]:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ"""
        opportunities = []
        
        try:
            # Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±
            ctr_opportunities = await self._detect_ctr_opportunities(campaigns, keywords)
            opportunities.extend(ctr_opportunities)
            
            # Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹Ø±ÙˆØ¶
            bid_opportunities = await self._detect_bid_opportunities(keywords)
            opportunities.extend(bid_opportunities)
            
            # Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ©
            negative_keyword_opportunities = await self._detect_negative_keyword_opportunities(keywords)
            opportunities.extend(negative_keyword_opportunities)
            
            # Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©
            budget_opportunities = await self._detect_budget_opportunities(campaigns)
            opportunities.extend(budget_opportunities)
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙØ±Øµ Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
            opportunities.sort(key=lambda x: x.impact_score, reverse=True)
            
            return opportunities
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ: {e}")
            return []
    
    async def _detect_ctr_opportunities(self, campaigns: List[CampaignInfo], keywords: List[KeywordInfo]) -> List[OpportunityInsight]:
        """Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±"""
        opportunities = []
        
        try:
            for campaign in campaigns:
                ctr = campaign.performance_metrics.get('ctr', 0)
                if ctr < self.opportunity_rules['low_ctr_threshold']:
                    opportunity = OpportunityInsight(
                        opportunity_id=generate_unique_id('opp_ctr') if SERVICES_STATUS['helpers'] else f"ctr_{campaign.campaign_id}",
                        type=OpportunityType.AD_COPY_IMPROVEMENT,
                        title=f"ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± Ù„Ù„Ø­Ù…Ù„Ø© {campaign.name}",
                        description=f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ {ctr:.2f}% Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨",
                        impact_score=85.0,
                        effort_score=60.0,
                        priority="high",
                        estimated_impact={
                            'ctr_improvement': f"+{2.0 - ctr:.1f}%",
                            'additional_clicks': int((2.0 - ctr) * campaign.performance_metrics.get('impressions', 0) / 100),
                            'potential_conversions': int((2.0 - ctr) * campaign.performance_metrics.get('impressions', 0) * campaign.performance_metrics.get('conversion_rate', 2) / 10000)
                        },
                        recommended_actions=[
                            "Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ù†ØµÙˆØµ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª",
                            "Ø¥Ø¶Ø§ÙØ© Ø¹Ø¨Ø§Ø±Ø§Øª Ø¯Ø¹ÙˆØ© Ù„Ù„Ø¹Ù…Ù„ Ù‚ÙˆÙŠØ©",
                            "Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ø®ØªÙ„ÙØ©",
                            "ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ ÙˆØ§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª"
                        ],
                        supporting_data={
                            'current_ctr': ctr,
                            'target_ctr': 2.0,
                            'campaign_id': campaign.campaign_id,
                            'impressions': campaign.performance_metrics.get('impressions', 0)
                        }
                    )
                    opportunities.append(opportunity)
            
            return opportunities
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ CTR: {e}")
            return []
    
    async def _detect_bid_opportunities(self, keywords: List[KeywordInfo]) -> List[OpportunityInsight]:
        """Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹Ø±ÙˆØ¶"""
        opportunities = []
        
        try:
            high_cpc_keywords = [kw for kw in keywords if kw.performance_metrics.get('avg_cpc', 0) > self.opportunity_rules['high_cpc_threshold']]
            
            if high_cpc_keywords:
                total_cost_savings = sum(kw.performance_metrics.get('cost', 0) * 0.2 for kw in high_cpc_keywords)
                
                opportunity = OpportunityInsight(
                    opportunity_id=generate_unique_id('opp_bid') if SERVICES_STATUS['helpers'] else f"bid_{int(time.time())}",
                    type=OpportunityType.BID_OPTIMIZATION,
                    title="ØªØ­Ø³ÙŠÙ† Ø¹Ø±ÙˆØ¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„ØªÙƒÙ„ÙØ©",
                    description=f"ÙŠÙˆØ¬Ø¯ {len(high_cpc_keywords)} ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø¨ØªÙƒÙ„ÙØ© Ù†Ù‚Ø±Ø© Ø¹Ø§Ù„ÙŠØ©",
                    impact_score=75.0,
                    effort_score=40.0,
                    priority="medium",
                    estimated_impact={
                        'cost_savings': f"{total_cost_savings:.2f} Ø±ÙŠØ§Ù„ Ø´Ù‡Ø±ÙŠØ§Ù‹",
                        'affected_keywords': len(high_cpc_keywords),
                        'average_cpc_reduction': "15-25%"
                    },
                    recommended_actions=[
                        "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¹Ø±ÙˆØ¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„ØªÙƒÙ„ÙØ©",
                        "ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø¹Ø±ÙˆØ¶ Ø°ÙƒÙŠØ©",
                        "ØªØ­Ø³ÙŠÙ† Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙƒÙ„ÙØ©",
                        "Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù"
                    ],
                    supporting_data={
                        'high_cpc_keywords': [kw.text for kw in high_cpc_keywords[:10]],
                        'average_cpc': np.mean([kw.performance_metrics.get('avg_cpc', 0) for kw in high_cpc_keywords]),
                        'total_keywords': len(high_cpc_keywords)
                    }
                )
                opportunities.append(opportunity)
            
            return opportunities
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ Ø§Ù„Ø¹Ø±ÙˆØ¶: {e}")
            return []
    
    async def _detect_negative_keyword_opportunities(self, keywords: List[KeywordInfo]) -> List[OpportunityInsight]:
        """Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ©"""
        opportunities = []
        
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ø¨Ù…Ø¹Ø¯Ù„ ØªØ­ÙˆÙŠÙ„ Ù…Ù†Ø®ÙØ¶ ÙˆØªÙƒÙ„ÙØ© Ø¹Ø§Ù„ÙŠØ©
            poor_performing_keywords = [
                kw for kw in keywords 
                if kw.performance_metrics.get('conversion_rate', 0) < 1.0 
                and kw.performance_metrics.get('cost', 0) > 50
            ]
            
            if poor_performing_keywords:
                potential_savings = sum(kw.performance_metrics.get('cost', 0) for kw in poor_performing_keywords)
                
                opportunity = OpportunityInsight(
                    opportunity_id=generate_unique_id('opp_neg') if SERVICES_STATUS['helpers'] else f"neg_{int(time.time())}",
                    type=OpportunityType.NEGATIVE_KEYWORDS,
                    title="Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù",
                    description=f"ÙŠÙˆØ¬Ø¯ {len(poor_performing_keywords)} ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø£Ø¯Ø§Ø¡ Ø¶Ø¹ÙŠÙ",
                    impact_score=70.0,
                    effort_score=30.0,
                    priority="medium",
                    estimated_impact={
                        'cost_savings': f"{potential_savings:.2f} Ø±ÙŠØ§Ù„ Ø´Ù‡Ø±ÙŠØ§Ù‹",
                        'improved_relevance': "ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø¨Ù†Ø³Ø¨Ø© 20-30%",
                        'affected_keywords': len(poor_performing_keywords)
                    },
                    recommended_actions=[
                        "ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø¶Ø¹ÙŠÙØ© Ø§Ù„Ø£Ø¯Ø§Ø¡",
                        "Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ØºÙŠØ± Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©",
                        "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ù†ÙˆØ§Ø¹ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©",
                        "ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±"
                    ],
                    supporting_data={
                        'poor_keywords': [kw.text for kw in poor_performing_keywords[:10]],
                        'average_conversion_rate': np.mean([kw.performance_metrics.get('conversion_rate', 0) for kw in poor_performing_keywords]),
                        'total_cost': potential_savings
                    }
                )
                opportunities.append(opportunity)
            
            return opportunities
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ©: {e}")
            return []
    
    async def _detect_budget_opportunities(self, campaigns: List[CampaignInfo]) -> List[OpportunityInsight]:
        """Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©"""
        opportunities = []
        
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ù…Ù„Ø§Øª
            campaign_performance = []
            for campaign in campaigns:
                performance_score = self.performance_analyzer.calculate_performance_score(campaign.performance_metrics)
                campaign_performance.append({
                    'campaign': campaign,
                    'performance_score': performance_score,
                    'budget': campaign.budget_amount or 0
                })
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡
            campaign_performance.sort(key=lambda x: x['performance_score'], reverse=True)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙØ±Øµ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ²ÙŠØ¹
            high_performers = [cp for cp in campaign_performance if cp['performance_score'] > 70]
            low_performers = [cp for cp in campaign_performance if cp['performance_score'] < 40]
            
            if high_performers and low_performers:
                opportunity = OpportunityInsight(
                    opportunity_id=generate_unique_id('opp_budget') if SERVICES_STATUS['helpers'] else f"budget_{int(time.time())}",
                    type=OpportunityType.BUDGET_REALLOCATION,
                    title="Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡",
                    description=f"ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ù…Ù† {len(low_performers)} Ø­Ù…Ù„Ø© Ø¶Ø¹ÙŠÙØ© Ø¥Ù„Ù‰ {len(high_performers)} Ø­Ù…Ù„Ø© Ù‚ÙˆÙŠØ©",
                    impact_score=80.0,
                    effort_score=50.0,
                    priority="high",
                    estimated_impact={
                        'roi_improvement': "15-25%",
                        'high_performing_campaigns': len(high_performers),
                        'underperforming_campaigns': len(low_performers),
                        'potential_budget_shift': f"{sum(cp['budget'] for cp in low_performers) * 0.3:.2f} Ø±ÙŠØ§Ù„"
                    },
                    recommended_actions=[
                        "Ø²ÙŠØ§Ø¯Ø© Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ø­Ù…Ù„Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¯Ø§Ø¡",
                        "ØªÙ‚Ù„ÙŠÙ„ Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ø­Ù…Ù„Ø§Øª Ø¶Ø¹ÙŠÙØ© Ø§Ù„Ø£Ø¯Ø§Ø¡",
                        "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø­Ù…Ù„Ø§Øª Ø¶Ø¹ÙŠÙØ© Ø§Ù„Ø£Ø¯Ø§Ø¡",
                        "ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©"
                    ],
                    supporting_data={
                        'high_performers': [cp['campaign'].name for cp in high_performers[:5]],
                        'low_performers': [cp['campaign'].name for cp in low_performers[:5]],
                        'performance_gap': campaign_performance[0]['performance_score'] - campaign_performance[-1]['performance_score']
                    }
                )
                opportunities.append(opportunity)
            
            return opportunities
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù ÙØ±Øµ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©: {e}")
            return []

class GoogleAdsDiscoveryService:
    """Ø®Ø¯Ù…Ø© Ø§ÙƒØªØ´Ø§Ù Google Ads Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© ÙˆØ§Ù„Ø°ÙƒÙŠØ©"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø®Ø¯Ù…Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù"""
        self.google_ads_client = GoogleAdsClient() if SERVICES_STATUS['google_ads_client'] else None
        self.db_manager = DatabaseManager() if SERVICES_STATUS['database'] else None
        self.ai_insights_engine = AIInsightsEngine()
        self.opportunity_detector = OpportunityDetector()
        self.performance_analyzer = PerformanceAnalyzer()
        
        # ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ù†ØªØ§Ø¦Ø¬
        self.discovery_cache = {}
        self.cache_expiry = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®Ø¯Ù…Ø©
        self.service_stats = {
            'total_discoveries': 0,
            'successful_discoveries': 0,
            'failed_discoveries': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'last_discovery': None
        }
        
        logger.info("ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø®Ø¯Ù…Ø© Ø§ÙƒØªØ´Ø§Ù Google Ads Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©")
    
    async def discover_accounts(self, config: DiscoveryConfig) -> Dict[str, Any]:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª"""
        try:
            self.service_stats['total_discoveries'] += 1
            
            # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            cache_key = f"accounts_{config.customer_id}"
            if config.enable_caching:
                cached_result = await self._get_from_cache(cache_key)
                if cached_result:
                    self.service_stats['cache_hits'] += 1
                    return cached_result
            
            self.service_stats['cache_misses'] += 1
            
            # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
            accounts = await self._fetch_accounts(config)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
            analysis = await self._analyze_accounts(accounts, config)
            
            result = {
                'success': True,
                'discovery_type': 'accounts',
                'config': asdict(config),
                'accounts': [asdict(account) for account in accounts],
                'analysis': analysis,
                'total_accounts': len(accounts),
                'discovery_timestamp': datetime.now(timezone.utc).isoformat(),
                'cache_key': cache_key
            }
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            if config.enable_caching:
                await self._save_to_cache(cache_key, result, config.cache_duration_hours)
            
            self.service_stats['successful_discoveries'] += 1
            self.service_stats['last_discovery'] = datetime.now(timezone.utc)
            
            return result
            
        except Exception as e:
            self.service_stats['failed_discoveries'] += 1
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª: {e}")
            return {'success': False, 'error': str(e)}
    
    async def discover_campaigns(self, config: DiscoveryConfig) -> Dict[str, Any]:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ù…Ù„Ø§Øª"""
        try:
            self.service_stats['total_discoveries'] += 1
            
            # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            cache_key = f"campaigns_{config.customer_id}_{config.analysis_depth.value}"
            if config.enable_caching:
                cached_result = await self._get_from_cache(cache_key)
                if cached_result:
                    self.service_stats['cache_hits'] += 1
                    return cached_result
            
            self.service_stats['cache_misses'] += 1
            
            # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ù…Ù„Ø§Øª
            campaigns = await self._fetch_campaigns(config)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ù…Ù„Ø§Øª
            analysis = await self._analyze_campaigns(campaigns, config)
            
            # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ
            opportunities = []
            if config.analysis_depth in [AnalysisDepth.ADVANCED, AnalysisDepth.COMPREHENSIVE]:
                keywords = await self._fetch_keywords_for_campaigns(campaigns, config)
                opportunities = await self.opportunity_detector.detect_opportunities(campaigns, keywords)
            
            result = {
                'success': True,
                'discovery_type': 'campaigns',
                'config': asdict(config),
                'campaigns': [asdict(campaign) for campaign in campaigns],
                'analysis': analysis,
                'opportunities': [asdict(opp) for opp in opportunities],
                'total_campaigns': len(campaigns),
                'total_opportunities': len(opportunities),
                'discovery_timestamp': datetime.now(timezone.utc).isoformat(),
                'cache_key': cache_key
            }
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            if config.enable_caching:
                await self._save_to_cache(cache_key, result, config.cache_duration_hours)
            
            self.service_stats['successful_discoveries'] += 1
            self.service_stats['last_discovery'] = datetime.now(timezone.utc)
            
            return result
            
        except Exception as e:
            self.service_stats['failed_discoveries'] += 1
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ù…Ù„Ø§Øª: {e}")
            return {'success': False, 'error': str(e)}
    
    async def discover_keywords(self, config: DiscoveryConfig) -> Dict[str, Any]:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        try:
            self.service_stats['total_discoveries'] += 1
            
            # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            cache_key = f"keywords_{config.customer_id}_{config.analysis_depth.value}"
            if config.enable_caching:
                cached_result = await self._get_from_cache(cache_key)
                if cached_result:
                    self.service_stats['cache_hits'] += 1
                    return cached_result
            
            self.service_stats['cache_misses'] += 1
            
            # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            keywords = await self._fetch_keywords(config)
            
            # ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
            ai_analysis = {}
            if config.include_ai_insights:
                ai_analysis = await self.ai_insights_engine.analyze_keywords(keywords)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
            performance_analysis = await self._analyze_keyword_performance(keywords, config)
            
            result = {
                'success': True,
                'discovery_type': 'keywords',
                'config': asdict(config),
                'keywords': [asdict(keyword) for keyword in keywords],
                'ai_analysis': ai_analysis,
                'performance_analysis': performance_analysis,
                'total_keywords': len(keywords),
                'discovery_timestamp': datetime.now(timezone.utc).isoformat(),
                'cache_key': cache_key
            }
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            if config.enable_caching:
                await self._save_to_cache(cache_key, result, config.cache_duration_hours)
            
            self.service_stats['successful_discoveries'] += 1
            self.service_stats['last_discovery'] = datetime.now(timezone.utc)
            
            return result
            
        except Exception as e:
            self.service_stats['failed_discoveries'] += 1
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {e}")
            return {'success': False, 'error': str(e)}
    
    async def comprehensive_discovery(self, config: DiscoveryConfig) -> Dict[str, Any]:
        """Ø§ÙƒØªØ´Ø§Ù Ø´Ø§Ù…Ù„"""
        try:
            self.service_stats['total_discoveries'] += 1
            
            # ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            tasks = []
            
            if DiscoveryType.ACCOUNTS in config.discovery_types:
                tasks.append(self.discover_accounts(config))
            
            if DiscoveryType.CAMPAIGNS in config.discovery_types:
                tasks.append(self.discover_campaigns(config))
            
            if DiscoveryType.KEYWORDS in config.discovery_types:
                tasks.append(self.discover_keywords(config))
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù… Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            comprehensive_result = {
                'success': True,
                'discovery_type': 'comprehensive',
                'config': asdict(config),
                'results': {},
                'summary': {},
                'discovery_timestamp': datetime.now(timezone.utc).isoformat()
            }
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù‡Ù…Ø© {i}: {result}")
                    continue
                
                if result.get('success'):
                    discovery_type = result.get('discovery_type')
                    comprehensive_result['results'][discovery_type] = result
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ
            comprehensive_result['summary'] = await self._create_comprehensive_summary(comprehensive_result['results'])
            
            self.service_stats['successful_discoveries'] += 1
            self.service_stats['last_discovery'] = datetime.now(timezone.utc)
            
            return comprehensive_result
            
        except Exception as e:
            self.service_stats['failed_discoveries'] += 1
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø§Ù…Ù„: {e}")
            return {'success': False, 'error': str(e)}
    
    # Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§ÙƒØªØ´Ø§Ù
    async def _fetch_accounts(self, config: DiscoveryConfig) -> List[AccountInfo]:
        """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Ads API
        accounts = [
            AccountInfo(
                customer_id=config.customer_id,
                name="Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ",
                currency_code=config.currency,
                time_zone=config.timezone,
                status="ENABLED",
                account_type="STANDARD",
                descriptive_name="Ø­Ø³Ø§Ø¨ Google Ads Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ",
                can_manage_clients=False,
                test_account=False,
                auto_tagging_enabled=True,
                created_date=datetime.now(timezone.utc) - timedelta(days=365)
            )
        ]
        return accounts
    
    async def _fetch_campaigns(self, config: DiscoveryConfig) -> List[CampaignInfo]:
        """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­Ù…Ù„Ø§Øª"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Ads API
        campaigns = [
            CampaignInfo(
                campaign_id="12345678901",
                name="Ø­Ù…Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
                status="ENABLED",
                campaign_type="SEARCH",
                serving_status="SERVING",
                start_date="2024-01-01",
                budget_amount=1000.0,
                budget_type="DAILY",
                bidding_strategy="TARGET_CPA",
                target_locations=["Saudi Arabia", "UAE"],
                target_languages=["ar", "en"],
                ad_groups_count=5,
                keywords_count=50,
                ads_count=15,
                performance_metrics={
                    'impressions': 10000,
                    'clicks': 500,
                    'ctr': 5.0,
                    'cost': 2500.0,
                    'conversions': 25,
                    'conversion_rate': 5.0,
                    'cost_per_conversion': 100.0,
                    'avg_cpc': 5.0
                }
            ),
            CampaignInfo(
                campaign_id="12345678902",
                name="Ø­Ù…Ù„Ø© Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†ÙŠØ©",
                status="ENABLED",
                campaign_type="DISPLAY",
                serving_status="SERVING",
                start_date="2024-02-01",
                budget_amount=500.0,
                budget_type="DAILY",
                bidding_strategy="TARGET_CPM",
                target_locations=["Saudi Arabia"],
                target_languages=["ar"],
                ad_groups_count=3,
                keywords_count=0,
                ads_count=8,
                performance_metrics={
                    'impressions': 50000,
                    'clicks': 200,
                    'ctr': 0.4,
                    'cost': 800.0,
                    'conversions': 8,
                    'conversion_rate': 4.0,
                    'cost_per_conversion': 100.0,
                    'avg_cpc': 4.0
                }
            )
        ]
        return campaigns
    
    async def _fetch_keywords(self, config: DiscoveryConfig) -> List[KeywordInfo]:
        """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Ads API
        keywords = [
            KeywordInfo(
                keyword_id="11111111111",
                text="Ø´Ø±Ø§Ø¡ Ø³ÙŠØ§Ø±Ø©",
                match_type="BROAD",
                status="ENABLED",
                bid_amount=5.0,
                quality_score=8,
                search_volume=1000,
                competition="MEDIUM",
                suggested_bid=4.5,
                performance_metrics={
                    'impressions': 2000,
                    'clicks': 100,
                    'ctr': 5.0,
                    'cost': 500.0,
                    'conversions': 5,
                    'conversion_rate': 5.0,
                    'avg_cpc': 5.0
                }
            ),
            KeywordInfo(
                keyword_id="11111111112",
                text="Ø³ÙŠØ§Ø±Ø§Øª Ù„Ù„Ø¨ÙŠØ¹",
                match_type="PHRASE",
                status="ENABLED",
                bid_amount=6.0,
                quality_score=7,
                search_volume=800,
                competition="HIGH",
                suggested_bid=7.0,
                performance_metrics={
                    'impressions': 1500,
                    'clicks': 75,
                    'ctr': 5.0,
                    'cost': 450.0,
                    'conversions': 4,
                    'conversion_rate': 5.3,
                    'avg_cpc': 6.0
                }
            ),
            KeywordInfo(
                keyword_id="11111111113",
                text="[Ø³ÙŠØ§Ø±Ø© Ø¬Ø¯ÙŠØ¯Ø©]",
                match_type="EXACT",
                status="ENABLED",
                bid_amount=8.0,
                quality_score=9,
                search_volume=500,
                competition="LOW",
                suggested_bid=6.0,
                performance_metrics={
                    'impressions': 800,
                    'clicks': 60,
                    'ctr': 7.5,
                    'cost': 480.0,
                    'conversions': 6,
                    'conversion_rate': 10.0,
                    'avg_cpc': 8.0
                }
            )
        ]
        return keywords
    
    async def _fetch_keywords_for_campaigns(self, campaigns: List[CampaignInfo], config: DiscoveryConfig) -> List[KeywordInfo]:
        """Ø¬Ù„Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø­Ù…Ù„Ø§Øª"""
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ÙŠØªÙ… Ø¬Ù„Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„ÙƒÙ„ Ø­Ù…Ù„Ø©
        return await self._fetch_keywords(config)
    
    async def _analyze_accounts(self, accounts: List[AccountInfo], config: DiscoveryConfig) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª"""
        analysis = {
            'total_accounts': len(accounts),
            'account_types': Counter([acc.account_type for acc in accounts]),
            'currencies': Counter([acc.currency_code for acc in accounts]),
            'timezones': Counter([acc.time_zone for acc in accounts]),
            'enabled_accounts': len([acc for acc in accounts if acc.status == 'ENABLED']),
            'test_accounts': len([acc for acc in accounts if acc.test_account]),
            'auto_tagging_enabled': len([acc for acc in accounts if acc.auto_tagging_enabled])
        }
        
        return analysis
    
    async def _analyze_campaigns(self, campaigns: List[CampaignInfo], config: DiscoveryConfig) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ù…Ù„Ø§Øª"""
        analysis = {
            'total_campaigns': len(campaigns),
            'campaign_types': Counter([camp.campaign_type for camp in campaigns]),
            'campaign_statuses': Counter([camp.status for camp in campaigns]),
            'serving_statuses': Counter([camp.serving_status for camp in campaigns]),
            'bidding_strategies': Counter([camp.bidding_strategy for camp in campaigns]),
            'total_budget': sum([camp.budget_amount or 0 for camp in campaigns]),
            'average_budget': np.mean([camp.budget_amount or 0 for camp in campaigns]),
            'total_ad_groups': sum([camp.ad_groups_count for camp in campaigns]),
            'total_keywords': sum([camp.keywords_count for camp in campaigns]),
            'total_ads': sum([camp.ads_count for camp in campaigns])
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if config.include_performance_metrics:
            performance_metrics = []
            for campaign in campaigns:
                if campaign.performance_metrics:
                    performance_score = self.performance_analyzer.calculate_performance_score(campaign.performance_metrics)
                    performance_metrics.append(performance_score)
            
            if performance_metrics:
                analysis['performance_analysis'] = {
                    'average_performance_score': np.mean(performance_metrics),
                    'best_performing_campaign': max(performance_metrics),
                    'worst_performing_campaign': min(performance_metrics),
                    'performance_distribution': {
                        'excellent': len([p for p in performance_metrics if p >= 80]),
                        'good': len([p for p in performance_metrics if 60 <= p < 80]),
                        'average': len([p for p in performance_metrics if 40 <= p < 60]),
                        'poor': len([p for p in performance_metrics if p < 40])
                    }
                }
        
        return analysis
    
    async def _analyze_keyword_performance(self, keywords: List[KeywordInfo], config: DiscoveryConfig) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        analysis = {
            'total_keywords': len(keywords),
            'match_types': Counter([kw.match_type for kw in keywords]),
            'statuses': Counter([kw.status for kw in keywords]),
            'competition_levels': Counter([kw.competition for kw in keywords if kw.competition])
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_scores = [kw.quality_score for kw in keywords if kw.quality_score]
        if quality_scores:
            analysis['quality_score_analysis'] = {
                'average_quality_score': np.mean(quality_scores),
                'quality_score_distribution': {
                    'excellent': len([q for q in quality_scores if q >= 8]),
                    'good': len([q for q in quality_scores if 6 <= q < 8]),
                    'average': len([q for q in quality_scores if 4 <= q < 6]),
                    'poor': len([q for q in quality_scores if q < 4])
                }
            }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if config.include_performance_metrics:
            performance_data = []
            for keyword in keywords:
                if keyword.performance_metrics:
                    performance_data.append(keyword.performance_metrics)
            
            if performance_data:
                analysis['performance_summary'] = {
                    'total_impressions': sum([p.get('impressions', 0) for p in performance_data]),
                    'total_clicks': sum([p.get('clicks', 0) for p in performance_data]),
                    'total_cost': sum([p.get('cost', 0) for p in performance_data]),
                    'total_conversions': sum([p.get('conversions', 0) for p in performance_data]),
                    'average_ctr': np.mean([p.get('ctr', 0) for p in performance_data]),
                    'average_cpc': np.mean([p.get('avg_cpc', 0) for p in performance_data]),
                    'average_conversion_rate': np.mean([p.get('conversion_rate', 0) for p in performance_data])
                }
        
        return analysis
    
    async def _create_comprehensive_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„"""
        summary = {
            'discovery_types_completed': list(results.keys()),
            'total_discovery_types': len(results),
            'overall_health_score': 0.0,
            'key_insights': [],
            'priority_recommendations': [],
            'next_steps': []
        }
        
        # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
        health_scores = []
        
        # ØªØ­Ù„ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø­Ù…Ù„Ø§Øª
        if 'campaigns' in results:
            campaigns_data = results['campaigns']
            if campaigns_data.get('analysis', {}).get('performance_analysis'):
                perf_analysis = campaigns_data['analysis']['performance_analysis']
                health_scores.append(perf_analysis.get('average_performance_score', 0))
                
                summary['key_insights'].append(
                    f"Ù…ØªÙˆØ³Ø· Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ù…Ù„Ø§Øª: {perf_analysis.get('average_performance_score', 0):.1f}%"
                )
        
        # ØªØ­Ù„ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        if 'keywords' in results:
            keywords_data = results['keywords']
            if keywords_data.get('performance_analysis', {}).get('quality_score_analysis'):
                quality_analysis = keywords_data['performance_analysis']['quality_score_analysis']
                avg_quality = quality_analysis.get('average_quality_score', 0)
                health_scores.append(avg_quality * 10)  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ©
                
                summary['key_insights'].append(
                    f"Ù…ØªÙˆØ³Ø· Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©: {avg_quality:.1f}/10"
                )
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¹Ø§Ù…Ø©
        if health_scores:
            summary['overall_health_score'] = np.mean(health_scores)
        
        # ØªÙˆØµÙŠØ§Øª Ø£ÙˆÙ„ÙˆÙŠØ©
        if summary['overall_health_score'] < 60:
            summary['priority_recommendations'].append("ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø­Ø³Ø§Ø¨")
            summary['next_steps'].append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù…Ù„Ø§Øª ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©")
        
        if 'campaigns' in results and results['campaigns'].get('opportunities'):
            opportunities = results['campaigns']['opportunities']
            high_priority_opps = [opp for opp in opportunities if opp.get('priority') == 'high']
            if high_priority_opps:
                summary['priority_recommendations'].append(f"ØªÙ†ÙÙŠØ° {len(high_priority_opps)} ÙØ±ØµØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©")
        
        return summary
    
    # Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
    async def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            # ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            if cache_key in self.cache_expiry:
                if datetime.now(timezone.utc) > self.cache_expiry[cache_key]:
                    # Ø§Ù†ØªÙ‡Øª Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                    if cache_key in self.discovery_cache:
                        del self.discovery_cache[cache_key]
                    del self.cache_expiry[cache_key]
                    return None
            
            # Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if cache_key in self.discovery_cache:
                return self.discovery_cache[cache_key]
            
            # Ø¬Ù„Ø¨ Ù…Ù† Redis Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            if SERVICES_STATUS['redis']:
                cached_data = cache_get(f"discovery:{cache_key}")
                if cached_data:
                    self.discovery_cache[cache_key] = cached_data
                    return cached_data
            
            return None
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            return None
    
    async def _save_to_cache(self, cache_key: str, data: Dict[str, Any], duration_hours: int):
        """Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.discovery_cache[cache_key] = data
            self.cache_expiry[cache_key] = datetime.now(timezone.utc) + timedelta(hours=duration_hours)
            
            # Ø­ÙØ¸ ÙÙŠ Redis Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            if SERVICES_STATUS['redis']:
                cache_set(f"discovery:{cache_key}", data, duration_hours * 3600)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
    
    def get_service_stats(self) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®Ø¯Ù…Ø©"""
        success_rate = 0
        if self.service_stats['total_discoveries'] > 0:
            success_rate = (self.service_stats['successful_discoveries'] / self.service_stats['total_discoveries']) * 100
        
        cache_hit_rate = 0
        total_cache_requests = self.service_stats['cache_hits'] + self.service_stats['cache_misses']
        if total_cache_requests > 0:
            cache_hit_rate = (self.service_stats['cache_hits'] / total_cache_requests) * 100
        
        return {
            **self.service_stats,
            'success_rate': success_rate,
            'cache_hit_rate': cache_hit_rate,
            'active_cache_entries': len(self.discovery_cache),
            'services_status': SERVICES_STATUS
        }

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø®Ø¯Ù…Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
discovery_service = GoogleAdsDiscoveryService()

# ===========================================
# API Routes - Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©
# ===========================================

@google_ads_discovery_bp.route('/accounts', methods=['POST'])
@jwt_required()
async def discover_accounts():
    """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        config = DiscoveryConfig(
            customer_id=data.get('customer_id', ''),
            analysis_depth=AnalysisDepth(data.get('analysis_depth', 'standard')),
            include_historical_data=data.get('include_historical_data', True),
            historical_days=data.get('historical_days', 90),
            enable_caching=data.get('enable_caching', True),
            cache_duration_hours=data.get('cache_duration_hours', 24)
        )
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if SERVICES_STATUS['validators']:
            if not validate_customer_id(config.customer_id):
                return jsonify({
                    'success': False,
                    'error': 'Ù…Ø¹Ø±Ù Ø§Ù„Ø¹Ù…ÙŠÙ„ ØºÙŠØ± ØµØ­ÙŠØ­'
                }), 400
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        result = await discovery_service.discover_accounts(config)
        
        return jsonify(result), 200 if result['success'] else 400
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª',
            'message': str(e)
        }), 500

@google_ads_discovery_bp.route('/campaigns', methods=['POST'])
@jwt_required()
async def discover_campaigns():
    """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ù…Ù„Ø§Øª"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        config = DiscoveryConfig(
            customer_id=data.get('customer_id', ''),
            analysis_depth=AnalysisDepth(data.get('analysis_depth', 'standard')),
            include_historical_data=data.get('include_historical_data', True),
            historical_days=data.get('historical_days', 90),
            include_ai_insights=data.get('include_ai_insights', True),
            include_performance_metrics=data.get('include_performance_metrics', True),
            enable_caching=data.get('enable_caching', True)
        )
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        result = await discovery_service.discover_campaigns(config)
        
        return jsonify(result), 200 if result['success'] else 400
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ù…Ù„Ø§Øª: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ù…Ù„Ø§Øª',
            'message': str(e)
        }), 500

@google_ads_discovery_bp.route('/keywords', methods=['POST'])
@jwt_required()
async def discover_keywords():
    """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        config = DiscoveryConfig(
            customer_id=data.get('customer_id', ''),
            analysis_depth=AnalysisDepth(data.get('analysis_depth', 'advanced')),
            include_ai_insights=data.get('include_ai_insights', True),
            include_performance_metrics=data.get('include_performance_metrics', True),
            max_results_per_type=data.get('max_results', 100),
            enable_caching=data.get('enable_caching', True)
        )
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        result = await discovery_service.discover_keywords(config)
        
        return jsonify(result), 200 if result['success'] else 400
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©',
            'message': str(e)
        }), 500

@google_ads_discovery_bp.route('/comprehensive', methods=['POST'])
@jwt_required()
async def comprehensive_discovery():
    """Ø§ÙƒØªØ´Ø§Ù Ø´Ø§Ù…Ù„"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        # ØªØ­Ø¯ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        discovery_types = []
        requested_types = data.get('discovery_types', ['accounts', 'campaigns', 'keywords'])
        
        type_mapping = {
            'accounts': DiscoveryType.ACCOUNTS,
            'campaigns': DiscoveryType.CAMPAIGNS,
            'keywords': DiscoveryType.KEYWORDS,
            'competitors': DiscoveryType.COMPETITORS,
            'opportunities': DiscoveryType.OPPORTUNITIES
        }
        
        for type_name in requested_types:
            if type_name in type_mapping:
                discovery_types.append(type_mapping[type_name])
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
        config = DiscoveryConfig(
            customer_id=data.get('customer_id', ''),
            discovery_types=discovery_types,
            analysis_depth=AnalysisDepth(data.get('analysis_depth', 'comprehensive')),
            include_historical_data=data.get('include_historical_data', True),
            historical_days=data.get('historical_days', 90),
            include_competitor_analysis=data.get('include_competitor_analysis', False),
            include_ai_insights=data.get('include_ai_insights', True),
            include_performance_metrics=data.get('include_performance_metrics', True),
            parallel_processing=data.get('parallel_processing', True),
            enable_caching=data.get('enable_caching', True)
        )
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø§Ù…Ù„
        result = await discovery_service.comprehensive_discovery(config)
        
        return jsonify(result), 200 if result['success'] else 400
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø§Ù…Ù„: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø§Ù…Ù„',
            'message': str(e)
        }), 500

@google_ads_discovery_bp.route('/opportunities', methods=['POST'])
@jwt_required()
async def detect_opportunities():
    """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        customer_id = data.get('customer_id', '')
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¤Ù‚ØªØ©
        config = DiscoveryConfig(customer_id=customer_id)
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        campaigns = await discovery_service._fetch_campaigns(config)
        keywords = await discovery_service._fetch_keywords(config)
        
        # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ
        opportunities = await discovery_service.opportunity_detector.detect_opportunities(campaigns, keywords)
        
        return jsonify({
            'success': True,
            'opportunities': [asdict(opp) for opp in opportunities],
            'total_opportunities': len(opportunities),
            'high_priority_count': len([opp for opp in opportunities if opp.priority == 'high']),
            'medium_priority_count': len([opp for opp in opportunities if opp.priority == 'medium']),
            'low_priority_count': len([opp for opp in opportunities if opp.priority == 'low']),
            'timestamp': datetime.now(timezone.utc).isoformat()
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ±Øµ',
            'message': str(e)
        }), 500

@google_ads_discovery_bp.route('/stats', methods=['GET'])
@jwt_required()
def get_discovery_stats():
    """Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø®Ø¯Ù…Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù"""
    try:
        stats = discovery_service.get_service_stats()
        
        return jsonify({
            'success': True,
            'stats': stats,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª',
            'message': str(e)
        }), 500

@google_ads_discovery_bp.route('/health', methods=['GET'])
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø®Ø¯Ù…Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù"""
    try:
        health_status = {
            'service': 'Google Ads Discovery',
            'status': 'healthy',
            'version': '2.1.0',
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'services_status': SERVICES_STATUS,
            'cache_entries': len(discovery_service.discovery_cache),
            'total_discoveries': discovery_service.service_stats['total_discoveries']
        }
        
        # ÙØ­Øµ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if not any(SERVICES_STATUS.values()):
            health_status['status'] = 'degraded'
            health_status['warning'] = 'Ø¨Ø¹Ø¶ Ø§Ù„Ø®Ø¯Ù…Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­Ø©'
        
        return jsonify(health_status)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØµØ­Ø©: {e}")
        return jsonify({
            'service': 'Google Ads Discovery',
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }), 500

# ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Blueprint
logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Google Ads Discovery Blueprint - Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ù…ØªØ§Ø­Ø©: {DISCOVERY_SERVICES_AVAILABLE}")
logger.info(f"ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª: {sum(SERVICES_STATUS.values())}/7 Ù…ØªØ§Ø­Ø©")

# ØªØµØ¯ÙŠØ± Blueprint ÙˆØ§Ù„ÙƒÙ„Ø§Ø³Ø§Øª
__all__ = [
    'google_ads_discovery_bp',
    'GoogleAdsDiscoveryService',
    'DiscoveryConfig',
    'AccountInfo',
    'CampaignInfo',
    'KeywordInfo',
    'OpportunityInsight',
    'CompetitorInfo',
    'DiscoveryType',
    'AnalysisDepth',
    'OpportunityType',
    'PerformanceAnalyzer',
    'AIInsightsEngine',
    'OpportunityDetector'
]

