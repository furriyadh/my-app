"""
Google Ads Sync Service
Ø®Ø¯Ù…Ø© Ù…Ø²Ø§Ù…Ù†Ø© Google Ads Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØ§Ù„Ø°ÙƒÙŠØ©

ÙŠÙˆÙØ± ÙˆØ¸Ø§Ø¦Ù Ù…Ø²Ø§Ù…Ù†Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ØªØ·ÙˆØ±Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Google Ads Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ:
- Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
- Ù…Ø²Ø§Ù…Ù†Ø© ØªØ¯Ø±ÙŠØ¬ÙŠØ© ÙˆÙ…ØªØ²Ø§ÙŠØ¯Ø©
- ÙƒØ´Ù Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ø°ÙƒÙŠ
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©
- Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
- Ù†Ø¸Ø§Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø°ÙƒÙŠ
- ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ API

Author: Google Ads AI Platform Team
Version: 2.1.0
Security Level: Enterprise
Performance: Optimized with Smart Sync
"""

import os
import asyncio
import aiohttp
import json
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any, Tuple, Union, Set, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from functools import wraps, lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict, Counter, deque
import hashlib
import uuid
import pickle
import gzip

# Flask imports
from flask import Blueprint, request, jsonify, current_app
from flask_jwt_extended import jwt_required, get_jwt_identity, get_jwt

# Third-party imports
import pandas as pd
import numpy as np

# Local imports
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
logger = logging.getLogger(__name__)

# Ø¥Ù†Ø´Ø§Ø¡ Blueprint Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
google_ads_sync_bp = Blueprint(
    'google_ads_sync',
    __name__,
    url_prefix='/api/google-ads/sync',
    static_folder=None,
    template_folder=None
)

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
SYNC_SERVICES_STATUS = {
    'google_ads_client': False,
    'oauth_manager': False,
    'database': False,
    'redis': False,
    'validators': False,
    'helpers': False,
    'queue_manager': False
}

try:
    from services.google_ads_client import GoogleAdsClient
    SYNC_SERVICES_STATUS['google_ads_client'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ GoogleAdsClient ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from routes.google_ads.oauth import oauth_manager
    SYNC_SERVICES_STATUS['oauth_manager'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ OAuth Manager ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.database import DatabaseManager
    SYNC_SERVICES_STATUS['database'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ DatabaseManager ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.redis_config import cache_set, cache_get, cache_delete
    SYNC_SERVICES_STATUS['redis'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Redis ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.validators import validate_customer_id, validate_sync_params
    SYNC_SERVICES_STATUS['validators'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Validators ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from utils.helpers import (
        generate_unique_id, sanitize_text, calculate_hash,
        format_timestamp, compress_data, decompress_data
    )
    SYNC_SERVICES_STATUS['helpers'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ Helpers ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

try:
    from services.queue_manager import QueueManager, TaskPriority
    SYNC_SERVICES_STATUS['queue_manager'] = True
except ImportError as e:
    logger.warning(f"âš ï¸ QueueManager ØºÙŠØ± Ù…ØªØ§Ø­: {e}")

# ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª
SYNC_SERVICES_AVAILABLE = any(SYNC_SERVICES_STATUS.values())
logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø®Ø¯Ù…Ø§Øª Sync - Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {sum(SYNC_SERVICES_STATUS.values())}/7")

# Ø¥Ø¹Ø¯Ø§Ø¯ Thread Pool Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
sync_executor = ThreadPoolExecutor(max_workers=30, thread_name_prefix="sync_worker")

class SyncType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    FULL = "full"
    INCREMENTAL = "incremental"
    REAL_TIME = "real_time"
    SELECTIVE = "selective"
    DELTA = "delta"

class SyncStatus(Enum):
    """Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"
    CANCELLED = "cancelled"

class SyncPriority(Enum):
    """Ø£ÙˆÙ„ÙˆÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    CRITICAL = 4
    EMERGENCY = 5

class ConflictResolution(Enum):
    """Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª"""
    LOCAL_WINS = "local_wins"
    REMOTE_WINS = "remote_wins"
    MERGE = "merge"
    MANUAL = "manual"
    TIMESTAMP_BASED = "timestamp_based"

class DataEntity(Enum):
    """ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    ACCOUNTS = "accounts"
    CAMPAIGNS = "campaigns"
    AD_GROUPS = "ad_groups"
    ADS = "ads"
    KEYWORDS = "keywords"
    EXTENSIONS = "extensions"
    AUDIENCES = "audiences"
    CONVERSIONS = "conversions"
    PERFORMANCE = "performance"

@dataclass
class SyncConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    customer_id: str
    sync_type: SyncType = SyncType.INCREMENTAL
    entities: List[DataEntity] = field(default_factory=lambda: [DataEntity.CAMPAIGNS])
    priority: SyncPriority = SyncPriority.NORMAL
    conflict_resolution: ConflictResolution = ConflictResolution.TIMESTAMP_BASED
    batch_size: int = 100
    max_retries: int = 3
    retry_delay: int = 5
    timeout_seconds: int = 300
    enable_compression: bool = True
    enable_validation: bool = True
    enable_backup: bool = True
    parallel_processing: bool = True
    max_parallel_workers: int = 10
    sync_interval_minutes: int = 60
    include_historical_data: bool = False
    historical_days: int = 30
    enable_real_time_updates: bool = False
    webhook_url: Optional[str] = None
    notification_email: Optional[str] = None

@dataclass
class SyncResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    sync_id: str
    status: SyncStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: Optional[float] = None
    entities_synced: Dict[str, int] = field(default_factory=dict)
    entities_failed: Dict[str, int] = field(default_factory=dict)
    conflicts_detected: int = 0
    conflicts_resolved: int = 0
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    data_size_mb: float = 0.0
    api_calls_made: int = 0
    rate_limit_hits: int = 0

@dataclass
class SyncJob:
    """Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    job_id: str
    config: SyncConfig
    status: SyncStatus = SyncStatus.PENDING
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    progress_percentage: float = 0.0
    current_entity: Optional[str] = None
    result: Optional[SyncResult] = None
    retry_count: int = 0
    last_error: Optional[str] = None

@dataclass
class DataChange:
    """ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    change_id: str
    entity_type: DataEntity
    entity_id: str
    change_type: str  # CREATE, UPDATE, DELETE
    old_data: Optional[Dict[str, Any]] = None
    new_data: Optional[Dict[str, Any]] = None
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    source: str = "google_ads"
    conflict: bool = False
    resolved: bool = False

class RateLimitManager:
    """Ù…Ø¯ÙŠØ± Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„"""
        self.api_calls = defaultdict(deque)
        self.rate_limits = {
            'default': {'calls': 10000, 'period': 3600},  # 10K calls per hour
            'search': {'calls': 15000, 'period': 3600},   # 15K search calls per hour
            'mutate': {'calls': 5000, 'period': 3600}     # 5K mutate calls per hour
        }
        self.backoff_delays = [1, 2, 4, 8, 16, 32]  # Exponential backoff
        self.current_backoff = {}
    
    async def check_rate_limit(self, api_type: str = 'default') -> bool:
        """ÙØ­Øµ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„"""
        try:
            now = time.time()
            rate_limit = self.rate_limits.get(api_type, self.rate_limits['default'])
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            cutoff_time = now - rate_limit['period']
            while self.api_calls[api_type] and self.api_calls[api_type][0] < cutoff_time:
                self.api_calls[api_type].popleft()
            
            # ÙØ­Øµ Ø§Ù„Ø­Ø¯
            if len(self.api_calls[api_type]) >= rate_limit['calls']:
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„: {e}")
            return True  # Ø§Ù„Ø³Ù…Ø§Ø­ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
    
    async def record_api_call(self, api_type: str = 'default'):
        """ØªØ³Ø¬ÙŠÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API"""
        try:
            now = time.time()
            self.api_calls[api_type].append(now)
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† backoff Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø¬Ø§Ø­
            if api_type in self.current_backoff:
                self.current_backoff[api_type] = 0
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API: {e}")
    
    async def handle_rate_limit_exceeded(self, api_type: str = 'default') -> int:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© ØªØ¬Ø§ÙˆØ² Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„"""
        try:
            # Ø²ÙŠØ§Ø¯Ø© backoff delay
            current_level = self.current_backoff.get(api_type, 0)
            if current_level < len(self.backoff_delays) - 1:
                self.current_backoff[api_type] = current_level + 1
            
            delay = self.backoff_delays[self.current_backoff[api_type]]
            logger.warning(f"ØªØ¬Ø§ÙˆØ² Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„ Ù„Ù€ {api_type}ØŒ Ø§Ù†ØªØ¸Ø§Ø± {delay} Ø«Ø§Ù†ÙŠØ©")
            
            return delay
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ØªØ¬Ø§ÙˆØ² Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„: {e}")
            return 60  # Ø§Ù†ØªØ¸Ø§Ø± Ø§ÙØªØ±Ø§Ø¶ÙŠ

class ConflictResolver:
    """Ø­Ù„Ø§Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø­Ù„Ø§Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª"""
        self.resolution_strategies = {
            ConflictResolution.LOCAL_WINS: self._local_wins,
            ConflictResolution.REMOTE_WINS: self._remote_wins,
            ConflictResolution.MERGE: self._merge_data,
            ConflictResolution.TIMESTAMP_BASED: self._timestamp_based,
            ConflictResolution.MANUAL: self._manual_resolution
        }
    
    async def resolve_conflict(self, change: DataChange, strategy: ConflictResolution) -> DataChange:
        """Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶"""
        try:
            if strategy in self.resolution_strategies:
                resolved_change = await self.resolution_strategies[strategy](change)
                resolved_change.resolved = True
                return resolved_change
            else:
                logger.error(f"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {strategy}")
                return change
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶: {e}")
            return change
    
    async def _local_wins(self, change: DataChange) -> DataChange:
        """Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        return change
    
    async def _remote_wins(self, change: DataChange) -> DataChange:
        """Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø¹ÙŠØ¯Ø©"""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø¹ÙŠØ¯Ø©
        if change.new_data:
            change.old_data = change.new_data
        return change
    
    async def _merge_data(self, change: DataChange) -> DataChange:
        """Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            if change.old_data and change.new_data:
                # Ø¯Ù…Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                merged_data = {**change.old_data}
                
                for key, value in change.new_data.items():
                    if key not in merged_data or merged_data[key] != value:
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
                        merged_data[key] = value
                
                change.new_data = merged_data
            
            return change
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return change
    
    async def _timestamp_based(self, change: DataChange) -> DataChange:
        """Ø­Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ"""
        try:
            if change.old_data and change.new_data:
                old_timestamp = change.old_data.get('last_modified_time')
                new_timestamp = change.new_data.get('last_modified_time')
                
                if old_timestamp and new_timestamp:
                    if new_timestamp > old_timestamp:
                        # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø£Ø­Ø¯Ø«
                        return await self._remote_wins(change)
                    else:
                        # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø£Ø­Ø¯Ø«
                        return await self._local_wins(change)
            
            return change
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ: {e}")
            return change
    
    async def _manual_resolution(self, change: DataChange) -> DataChange:
        """Ø­Ù„ ÙŠØ¯ÙˆÙŠ"""
        # ÙˆØ¶Ø¹ Ø¹Ù„Ø§Ù…Ø© Ù„Ù„Ø­Ù„ Ø§Ù„ÙŠØ¯ÙˆÙŠ
        change.conflict = True
        change.resolved = False
        return change

class ChangeDetector:
    """ÙƒØ§Ø´Ù Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´Ù Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª"""
        self.entity_hashes = {}
        self.last_sync_timestamps = {}
    
    async def detect_changes(self, entity_type: DataEntity, current_data: List[Dict[str, Any]], 
                           last_sync_time: Optional[datetime] = None) -> List[DataChange]:
        """ÙƒØ´Ù Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª"""
        changes = []
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
            stored_data = await self._get_stored_data(entity_type)
            stored_hashes = self.entity_hashes.get(entity_type.value, {})
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            current_index = {item.get('id', str(i)): item for i, item in enumerate(current_data)}
            stored_index = {item.get('id', str(i)): item for i, item in enumerate(stored_data)}
            
            # ÙƒØ´Ù Ø§Ù„Ø¥Ø¶Ø§ÙØ§Øª ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª
            for entity_id, current_item in current_index.items():
                current_hash = self._calculate_hash(current_item)
                stored_hash = stored_hashes.get(entity_id)
                
                if entity_id not in stored_index:
                    # Ø¥Ø¶Ø§ÙØ© Ø¬Ø¯ÙŠØ¯Ø©
                    change = DataChange(
                        change_id=generate_unique_id('change') if SYNC_SERVICES_STATUS['helpers'] else f"change_{int(time.time())}",
                        entity_type=entity_type,
                        entity_id=entity_id,
                        change_type='CREATE',
                        new_data=current_item
                    )
                    changes.append(change)
                    
                elif stored_hash != current_hash:
                    # ØªØ­Ø¯ÙŠØ«
                    change = DataChange(
                        change_id=generate_unique_id('change') if SYNC_SERVICES_STATUS['helpers'] else f"change_{int(time.time())}",
                        entity_type=entity_type,
                        entity_id=entity_id,
                        change_type='UPDATE',
                        old_data=stored_index[entity_id],
                        new_data=current_item
                    )
                    changes.append(change)
            
            # ÙƒØ´Ù Ø§Ù„Ø­Ø°Ù
            for entity_id, stored_item in stored_index.items():
                if entity_id not in current_index:
                    change = DataChange(
                        change_id=generate_unique_id('change') if SYNC_SERVICES_STATUS['helpers'] else f"change_{int(time.time())}",
                        entity_type=entity_type,
                        entity_id=entity_id,
                        change_type='DELETE',
                        old_data=stored_item
                    )
                    changes.append(change)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‡Ø§Ø´Ø§Øª
            self.entity_hashes[entity_type.value] = {
                entity_id: self._calculate_hash(item) 
                for entity_id, item in current_index.items()
            }
            
            return changes
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª: {e}")
            return []
    
    def _calculate_hash(self, data: Dict[str, Any]) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù†Øµ Ù…Ø±ØªØ¨
            sorted_data = json.dumps(data, sort_keys=True, default=str)
            return hashlib.md5(sorted_data.encode()).hexdigest()
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ hash: {e}")
            return str(hash(str(data)))
    
    async def _get_stored_data(self, entity_type: DataEntity) -> List[Dict[str, Any]]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©"""
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if SYNC_SERVICES_STATUS['database']:
                # Ø¬Ù„Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                pass
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…Ù† Redis
            if SYNC_SERVICES_STATUS['redis']:
                cached_data = cache_get(f"sync_data:{entity_type.value}")
                if cached_data:
                    return cached_data
            
            return []
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©: {e}")
            return []

class SyncEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…ØªØ·ÙˆØ±"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
        self.google_ads_client = GoogleAdsClient() if SYNC_SERVICES_STATUS['google_ads_client'] else None
        self.db_manager = DatabaseManager() if SYNC_SERVICES_STATUS['database'] else None
        self.rate_limit_manager = RateLimitManager()
        self.conflict_resolver = ConflictResolver()
        self.change_detector = ChangeDetector()
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù‡Ø§Ù…
        self.active_jobs = {}
        self.job_queue = deque()
        self.completed_jobs = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.sync_stats = {
            'total_syncs': 0,
            'successful_syncs': 0,
            'failed_syncs': 0,
            'total_entities_synced': 0,
            'total_conflicts_resolved': 0,
            'total_api_calls': 0,
            'total_data_synced_mb': 0.0,
            'average_sync_duration': 0.0,
            'last_sync_time': None
        }
        
        # Ø®ÙŠÙˆØ· Ø§Ù„Ø¹Ù…Ù„
        self.worker_threads = []
        self.shutdown_event = threading.Event()
        
        logger.info("ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…ØªØ·ÙˆØ±")
    
    async def start_sync(self, config: SyncConfig) -> str:
        """Ø¨Ø¯Ø¡ Ù…Ø²Ø§Ù…Ù†Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ù…Ø© Ù…Ø²Ø§Ù…Ù†Ø©
            job_id = generate_unique_id('sync_job') if SYNC_SERVICES_STATUS['helpers'] else f"sync_{int(time.time())}"
            
            job = SyncJob(
                job_id=job_id,
                config=config,
                status=SyncStatus.PENDING
            )
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø·Ø§Ø¨ÙˆØ±
            self.job_queue.append(job)
            self.active_jobs[job_id] = job
            
            # Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            if config.parallel_processing:
                asyncio.create_task(self._process_sync_job(job))
            else:
                await self._process_sync_job(job)
            
            self.sync_stats['total_syncs'] += 1
            
            return job_id
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
            raise
    
    async def _process_sync_job(self, job: SyncJob):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
        try:
            job.status = SyncStatus.RUNNING
            job.started_at = datetime.now(timezone.utc)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
            result = SyncResult(
                sync_id=job.job_id,
                status=SyncStatus.RUNNING,
                start_time=job.started_at
            )
            
            job.result = result
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            if job.config.sync_type == SyncType.FULL:
                await self._full_sync(job)
            elif job.config.sync_type == SyncType.INCREMENTAL:
                await self._incremental_sync(job)
            elif job.config.sync_type == SyncType.REAL_TIME:
                await self._real_time_sync(job)
            elif job.config.sync_type == SyncType.SELECTIVE:
                await self._selective_sync(job)
            elif job.config.sync_type == SyncType.DELTA:
                await self._delta_sync(job)
            
            # Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
            job.status = SyncStatus.COMPLETED
            job.completed_at = datetime.now(timezone.utc)
            result.status = SyncStatus.COMPLETED
            result.end_time = job.completed_at
            result.duration_seconds = (job.completed_at - job.started_at).total_seconds()
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.sync_stats['successful_syncs'] += 1
            self.sync_stats['last_sync_time'] = job.completed_at
            
            # Ù†Ù‚Ù„ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©
            self.completed_jobs[job.job_id] = job
            if job.job_id in self.active_jobs:
                del self.active_jobs[job.job_id]
            
            logger.info(f"âœ… ØªÙ…Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¨Ù†Ø¬Ø§Ø­: {job.job_id}")
            
        except Exception as e:
            job.status = SyncStatus.FAILED
            job.last_error = str(e)
            if job.result:
                job.result.status = SyncStatus.FAILED
                job.result.errors.append(str(e))
            
            self.sync_stats['failed_syncs'] += 1
            logger.error(f"âŒ ÙØ´Ù„Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {job.job_id} - {e}")
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if job.retry_count < job.config.max_retries:
                job.retry_count += 1
                await asyncio.sleep(job.config.retry_delay)
                await self._process_sync_job(job)
    
    async def _full_sync(self, job: SyncJob):
        """Ù…Ø²Ø§Ù…Ù†Ø© ÙƒØ§Ù…Ù„Ø©"""
        try:
            for entity in job.config.entities:
                job.current_entity = entity.value
                
                # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                data = await self._fetch_entity_data(entity, job.config)
                
                # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                await self._save_entity_data(entity, data, job)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
                progress = (job.config.entities.index(entity) + 1) / len(job.config.entities) * 100
                job.progress_percentage = progress
                
                if job.result:
                    job.result.entities_synced[entity.value] = len(data)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: {e}")
            raise
    
    async def _incremental_sync(self, job: SyncJob):
        """Ù…Ø²Ø§Ù…Ù†Ø© ØªØ¯Ø±ÙŠØ¬ÙŠØ©"""
        try:
            for entity in job.config.entities:
                job.current_entity = entity.value
                
                # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø«Ø© ÙÙ‚Ø·
                last_sync_time = await self._get_last_sync_time(entity, job.config.customer_id)
                data = await self._fetch_entity_data(entity, job.config, since=last_sync_time)
                
                # ÙƒØ´Ù Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
                changes = await self.change_detector.detect_changes(entity, data, last_sync_time)
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
                await self._process_changes(changes, job)
                
                # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± Ù…Ø²Ø§Ù…Ù†Ø©
                await self._update_last_sync_time(entity, job.config.customer_id)
                
                if job.result:
                    job.result.entities_synced[entity.value] = len(changes)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ©: {e}")
            raise
    
    async def _real_time_sync(self, job: SyncJob):
        """Ù…Ø²Ø§Ù…Ù†Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ"""
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ webhook Ù„Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„ÙÙˆØ±ÙŠØ©
            if job.config.webhook_url:
                await self._setup_webhook(job.config.webhook_url, job.config.customer_id)
            
            # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©
            while job.status == SyncStatus.RUNNING:
                for entity in job.config.entities:
                    # ÙØ­Øµ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                    updates = await self._check_for_updates(entity, job.config.customer_id)
                    
                    if updates:
                        await self._process_real_time_updates(updates, job)
                
                # Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„ÙØ­Øµ Ø§Ù„ØªØ§Ù„ÙŠ
                await asyncio.sleep(job.config.sync_interval_minutes * 60)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙÙˆØ±ÙŠØ©: {e}")
            raise
    
    async def _selective_sync(self, job: SyncJob):
        """Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠØ©"""
        try:
            # Ù…Ø²Ø§Ù…Ù†Ø© ÙƒÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø·
            for entity in job.config.entities:
                job.current_entity = entity.value
                
                # ØªØ·Ø¨ÙŠÙ‚ ÙÙ„Ø§ØªØ± Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠØ©
                filtered_data = await self._fetch_filtered_data(entity, job.config)
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©
                await self._save_entity_data(entity, filtered_data, job)
                
                if job.result:
                    job.result.entities_synced[entity.value] = len(filtered_data)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠØ©: {e}")
            raise
    
    async def _delta_sync(self, job: SyncJob):
        """Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª"""
        try:
            for entity in job.config.entities:
                job.current_entity = entity.value
                
                # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ù…Ø­ÙÙˆØ¸Ø©
                current_data = await self._fetch_entity_data(entity, job.config)
                stored_data = await self.change_detector._get_stored_data(entity)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª
                deltas = await self._calculate_deltas(current_data, stored_data)
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª
                await self._apply_deltas(deltas, job)
                
                if job.result:
                    job.result.entities_synced[entity.value] = len(deltas)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª: {e}")
            raise
    
    async def _fetch_entity_data(self, entity: DataEntity, config: SyncConfig, 
                                since: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙŠØ§Ù†"""
        try:
            # ÙØ­Øµ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„
            if not await self.rate_limit_manager.check_rate_limit():
                delay = await self.rate_limit_manager.handle_rate_limit_exceeded()
                await asyncio.sleep(delay)
            
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Ads API
            data = []
            
            if entity == DataEntity.CAMPAIGNS:
                data = [
                    {
                        'id': '12345678901',
                        'name': 'Ø­Ù…Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©',
                        'status': 'ENABLED',
                        'type': 'SEARCH',
                        'budget': 1000.0,
                        'last_modified_time': datetime.now(timezone.utc).isoformat()
                    },
                    {
                        'id': '12345678902',
                        'name': 'Ø­Ù…Ù„Ø© Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†ÙŠØ©',
                        'status': 'ENABLED',
                        'type': 'DISPLAY',
                        'budget': 500.0,
                        'last_modified_time': datetime.now(timezone.utc).isoformat()
                    }
                ]
            elif entity == DataEntity.KEYWORDS:
                data = [
                    {
                        'id': '11111111111',
                        'text': 'Ø´Ø±Ø§Ø¡ Ø³ÙŠØ§Ø±Ø©',
                        'match_type': 'BROAD',
                        'status': 'ENABLED',
                        'bid': 5.0,
                        'last_modified_time': datetime.now(timezone.utc).isoformat()
                    },
                    {
                        'id': '11111111112',
                        'text': 'Ø³ÙŠØ§Ø±Ø§Øª Ù„Ù„Ø¨ÙŠØ¹',
                        'match_type': 'PHRASE',
                        'status': 'ENABLED',
                        'bid': 6.0,
                        'last_modified_time': datetime.now(timezone.utc).isoformat()
                    }
                ]
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API
            await self.rate_limit_manager.record_api_call()
            self.sync_stats['total_api_calls'] += 1
            
            return data
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {entity.value}: {e}")
            return []
    
    async def _save_entity_data(self, entity: DataEntity, data: List[Dict[str, Any]], job: SyncJob):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙŠØ§Ù†"""
        try:
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if SYNC_SERVICES_STATUS['database'] and self.db_manager:
                await self._save_to_database(entity, data, job.config.customer_id)
            
            # Ø­ÙØ¸ ÙÙŠ Redis Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            if SYNC_SERVICES_STATUS['redis']:
                cache_key = f"sync_data:{entity.value}:{job.config.customer_id}"
                cache_set(cache_key, data, 3600)  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
            
            # Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if job.config.enable_compression:
                compressed_size = await self._compress_and_store(entity, data, job.config.customer_id)
                if job.result:
                    job.result.data_size_mb += compressed_size
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
            if job.config.enable_backup:
                await self._create_backup(entity, data, job.config.customer_id)
            
            self.sync_stats['total_entities_synced'] += len(data)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª {entity.value}: {e}")
            if job.result:
                job.result.entities_failed[entity.value] = job.result.entities_failed.get(entity.value, 0) + 1
                job.result.errors.append(f"ÙØ´Ù„ Ø­ÙØ¸ {entity.value}: {str(e)}")
    
    async def _process_changes(self, changes: List[DataChange], job: SyncJob):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª"""
        try:
            conflicts_detected = 0
            conflicts_resolved = 0
            
            for change in changes:
                # ÙØ­Øµ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª
                if await self._detect_conflict(change):
                    change.conflict = True
                    conflicts_detected += 1
                    
                    # Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶
                    resolved_change = await self.conflict_resolver.resolve_conflict(
                        change, job.config.conflict_resolution
                    )
                    
                    if resolved_change.resolved:
                        conflicts_resolved += 1
                        await self._apply_change(resolved_change)
                    else:
                        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø­Ù„ Ø§Ù„ÙŠØ¯ÙˆÙŠ
                        await self._queue_for_manual_resolution(resolved_change)
                else:
                    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØºÙŠÙŠØ± Ù…Ø¨Ø§Ø´Ø±Ø©
                    await self._apply_change(change)
            
            if job.result:
                job.result.conflicts_detected += conflicts_detected
                job.result.conflicts_resolved += conflicts_resolved
            
            self.sync_stats['total_conflicts_resolved'] += conflicts_resolved
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª: {e}")
            raise
    
    async def _detect_conflict(self, change: DataChange) -> bool:
        """ÙƒØ´Ù Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª"""
        try:
            # ÙØ­Øµ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
            if change.change_type == 'UPDATE' and change.old_data and change.new_data:
                # ÙØ­Øµ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
                stored_timestamp = change.old_data.get('last_modified_time')
                new_timestamp = change.new_data.get('last_modified_time')
                
                if stored_timestamp and new_timestamp:
                    # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØ­Ø¯ÙŠØ« Ø¢Ø®Ø± ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
                    time_diff = abs((datetime.fromisoformat(new_timestamp.replace('Z', '+00:00')) - 
                                   datetime.fromisoformat(stored_timestamp.replace('Z', '+00:00'))).total_seconds())
                    
                    if time_diff < 60:  # Ø£Ù‚Ù„ Ù…Ù† Ø¯Ù‚ÙŠÙ‚Ø©
                        return True
            
            return False
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„ØªØ¹Ø§Ø±Ø¶: {e}")
            return False
    
    async def _apply_change(self, change: DataChange):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØºÙŠÙŠØ±"""
        try:
            if change.change_type == 'CREATE':
                await self._create_entity(change)
            elif change.change_type == 'UPDATE':
                await self._update_entity(change)
            elif change.change_type == 'DELETE':
                await self._delete_entity(change)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØºÙŠÙŠØ±: {e}")
            raise
    
    async def _create_entity(self, change: DataChange):
        """Ø¥Ù†Ø´Ø§Ø¡ ÙƒÙŠØ§Ù† Ø¬Ø¯ÙŠØ¯"""
        # ØªÙ†ÙÙŠØ° Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙƒÙŠØ§Ù†
        pass
    
    async def _update_entity(self, change: DataChange):
        """ØªØ­Ø¯ÙŠØ« ÙƒÙŠØ§Ù† Ù…ÙˆØ¬ÙˆØ¯"""
        # ØªÙ†ÙÙŠØ° ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙƒÙŠØ§Ù†
        pass
    
    async def _delete_entity(self, change: DataChange):
        """Ø­Ø°Ù ÙƒÙŠØ§Ù†"""
        # ØªÙ†ÙÙŠØ° Ø­Ø°Ù Ø§Ù„ÙƒÙŠØ§Ù†
        pass
    
    async def _queue_for_manual_resolution(self, change: DataChange):
        """Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø­Ù„ Ø§Ù„ÙŠØ¯ÙˆÙŠ"""
        try:
            # Ø­ÙØ¸ Ø§Ù„ØªØ¹Ø§Ø±Ø¶ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©
            if SYNC_SERVICES_STATUS['database'] and self.db_manager:
                # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                pass
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if change.entity_type:  # Ø¥Ø´Ø¹Ø§Ø± Ù…Ø¤Ù‚Øª
                logger.warning(f"ØªØ¹Ø§Ø±Ø¶ ÙŠØªØ·Ù„Ø¨ Ø­Ù„ ÙŠØ¯ÙˆÙŠ: {change.change_id}")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¹Ø§Ø±Ø¶ Ù„Ù„Ø­Ù„ Ø§Ù„ÙŠØ¯ÙˆÙŠ: {e}")
    
    # Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¥Ø¶Ø§ÙÙŠØ©
    async def _get_last_sync_time(self, entity: DataEntity, customer_id: str) -> Optional[datetime]:
        """Ø¬Ù„Ø¨ ÙˆÙ‚Øª Ø¢Ø®Ø± Ù…Ø²Ø§Ù…Ù†Ø©"""
        try:
            if SYNC_SERVICES_STATUS['redis']:
                timestamp = cache_get(f"last_sync:{entity.value}:{customer_id}")
                if timestamp:
                    return datetime.fromisoformat(timestamp)
            
            return None
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ ÙˆÙ‚Øª Ø¢Ø®Ø± Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
            return None
    
    async def _update_last_sync_time(self, entity: DataEntity, customer_id: str):
        """ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± Ù…Ø²Ø§Ù…Ù†Ø©"""
        try:
            now = datetime.now(timezone.utc)
            
            if SYNC_SERVICES_STATUS['redis']:
                cache_key = f"last_sync:{entity.value}:{customer_id}"
                cache_set(cache_key, now.isoformat(), 86400)  # 24 Ø³Ø§Ø¹Ø©
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
    
    async def _save_to_database(self, entity: DataEntity, data: List[Dict[str, Any]], customer_id: str):
        """Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # ØªÙ†ÙÙŠØ° Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        pass
    
    async def _compress_and_store(self, entity: DataEntity, data: List[Dict[str, Any]], customer_id: str) -> float:
        """Ø¶ØºØ· ÙˆØ­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ JSON
            json_data = json.dumps(data, default=str)
            
            # Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            compressed_data = gzip.compress(json_data.encode())
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¬Ù… Ø¨Ø§Ù„Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª
            size_mb = len(compressed_data) / (1024 * 1024)
            
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¶ØºÙˆØ·Ø©
            if SYNC_SERVICES_STATUS['redis']:
                cache_key = f"compressed_sync:{entity.value}:{customer_id}"
                cache_set(cache_key, compressed_data, 86400)
            
            return size_mb
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return 0.0
    
    async def _create_backup(self, entity: DataEntity, data: List[Dict[str, Any]], customer_id: str):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        try:
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            backup_key = f"backup:{entity.value}:{customer_id}:{timestamp}"
            
            if SYNC_SERVICES_STATUS['redis']:
                cache_set(backup_key, data, 604800)  # Ø£Ø³Ø¨ÙˆØ¹ ÙˆØ§Ø­Ø¯
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
    
    def get_sync_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Ø¬Ù„Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
        try:
            if job_id in self.active_jobs:
                job = self.active_jobs[job_id]
                return {
                    'job_id': job.job_id,
                    'status': job.status.value,
                    'progress': job.progress_percentage,
                    'current_entity': job.current_entity,
                    'created_at': job.created_at.isoformat(),
                    'started_at': job.started_at.isoformat() if job.started_at else None,
                    'retry_count': job.retry_count,
                    'last_error': job.last_error
                }
            elif job_id in self.completed_jobs:
                job = self.completed_jobs[job_id]
                return {
                    'job_id': job.job_id,
                    'status': job.status.value,
                    'progress': 100.0,
                    'completed_at': job.completed_at.isoformat() if job.completed_at else None,
                    'result': asdict(job.result) if job.result else None
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
            return None
    
    def get_sync_stats(self) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
        try:
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
            completed_jobs_list = list(self.completed_jobs.values())
            if completed_jobs_list:
                durations = [
                    job.result.duration_seconds for job in completed_jobs_list 
                    if job.result and job.result.duration_seconds
                ]
                if durations:
                    self.sync_stats['average_sync_duration'] = np.mean(durations)
            
            return {
                **self.sync_stats,
                'active_jobs': len(self.active_jobs),
                'queued_jobs': len(self.job_queue),
                'completed_jobs': len(self.completed_jobs),
                'services_status': SYNC_SERVICES_STATUS,
                'last_updated': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
            return self.sync_stats

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
sync_engine = SyncEngine()

# ===========================================
# API Routes - Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©
# ===========================================

@google_ads_sync_bp.route('/start', methods=['POST'])
@jwt_required()
async def start_sync():
    """Ø¨Ø¯Ø¡ Ù…Ø²Ø§Ù…Ù†Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json() or {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
        entities = []
        entity_names = data.get('entities', ['campaigns'])
        
        entity_mapping = {
            'accounts': DataEntity.ACCOUNTS,
            'campaigns': DataEntity.CAMPAIGNS,
            'ad_groups': DataEntity.AD_GROUPS,
            'ads': DataEntity.ADS,
            'keywords': DataEntity.KEYWORDS,
            'extensions': DataEntity.EXTENSIONS,
            'audiences': DataEntity.AUDIENCES,
            'conversions': DataEntity.CONVERSIONS,
            'performance': DataEntity.PERFORMANCE
        }
        
        for entity_name in entity_names:
            if entity_name in entity_mapping:
                entities.append(entity_mapping[entity_name])
        
        config = SyncConfig(
            customer_id=data.get('customer_id', ''),
            sync_type=SyncType(data.get('sync_type', 'incremental')),
            entities=entities,
            priority=SyncPriority(data.get('priority', 2)),
            conflict_resolution=ConflictResolution(data.get('conflict_resolution', 'timestamp_based')),
            batch_size=data.get('batch_size', 100),
            max_retries=data.get('max_retries', 3),
            timeout_seconds=data.get('timeout_seconds', 300),
            enable_compression=data.get('enable_compression', True),
            enable_validation=data.get('enable_validation', True),
            enable_backup=data.get('enable_backup', True),
            parallel_processing=data.get('parallel_processing', True),
            include_historical_data=data.get('include_historical_data', False),
            historical_days=data.get('historical_days', 30)
        )
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if SYNC_SERVICES_STATUS['validators']:
            if not validate_customer_id(config.customer_id):
                return jsonify({
                    'success': False,
                    'error': 'Ù…Ø¹Ø±Ù Ø§Ù„Ø¹Ù…ÙŠÙ„ ØºÙŠØ± ØµØ­ÙŠØ­'
                }), 400
        
        # Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
        job_id = await sync_engine.start_sync(config)
        
        return jsonify({
            'success': True,
            'job_id': job_id,
            'message': 'ØªÙ… Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¨Ù†Ø¬Ø§Ø­',
            'config': asdict(config)
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©',
            'message': str(e)
        }), 500

@google_ads_sync_bp.route('/status/<job_id>', methods=['GET'])
@jwt_required()
def get_sync_status(job_id: str):
    """Ø¬Ù„Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    try:
        status = sync_engine.get_sync_status(job_id)
        
        if status:
            return jsonify({
                'success': True,
                'status': status
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©'
            }), 404
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©',
            'message': str(e)
        }), 500

@google_ads_sync_bp.route('/cancel/<job_id>', methods=['POST'])
@jwt_required()
def cancel_sync(job_id: str):
    """Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    try:
        if job_id in sync_engine.active_jobs:
            job = sync_engine.active_jobs[job_id]
            job.status = SyncStatus.CANCELLED
            
            return jsonify({
                'success': True,
                'message': 'ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¨Ù†Ø¬Ø§Ø­'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£Ùˆ Ù…ÙƒØªÙ…Ù„Ø©'
            }), 404
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©',
            'message': str(e)
        }), 500

@google_ads_sync_bp.route('/history', methods=['GET'])
@jwt_required()
def get_sync_history():
    """Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    try:
        user_id = get_jwt_identity()
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©
        completed_jobs = []
        for job in sync_engine.completed_jobs.values():
            completed_jobs.append({
                'job_id': job.job_id,
                'status': job.status.value,
                'sync_type': job.config.sync_type.value,
                'entities': [entity.value for entity in job.config.entities],
                'created_at': job.created_at.isoformat(),
                'completed_at': job.completed_at.isoformat() if job.completed_at else None,
                'duration_seconds': job.result.duration_seconds if job.result else None,
                'entities_synced': job.result.entities_synced if job.result else {},
                'conflicts_resolved': job.result.conflicts_resolved if job.result else 0
            })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
        completed_jobs.sort(key=lambda x: x['created_at'], reverse=True)
        
        return jsonify({
            'success': True,
            'history': completed_jobs,
            'total_jobs': len(completed_jobs)
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©',
            'message': str(e)
        }), 500

@google_ads_sync_bp.route('/stats', methods=['GET'])
@jwt_required()
def get_sync_stats():
    """Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    try:
        stats = sync_engine.get_sync_stats()
        
        return jsonify({
            'success': True,
            'stats': stats,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©',
            'message': str(e)
        }), 500

@google_ads_sync_bp.route('/conflicts', methods=['GET'])
@jwt_required()
def get_conflicts():
    """Ø¬Ù„Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©"""
    try:
        # Ø¬Ù„Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Redis
        conflicts = []
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª
        conflicts = [
            {
                'conflict_id': 'conflict_001',
                'entity_type': 'campaigns',
                'entity_id': '12345678901',
                'change_type': 'UPDATE',
                'created_at': datetime.now(timezone.utc).isoformat(),
                'status': 'pending',
                'local_data': {'name': 'Ø­Ù…Ù„Ø© Ù…Ø­Ù„ÙŠØ©', 'budget': 1000},
                'remote_data': {'name': 'Ø­Ù…Ù„Ø© Ø¨Ø¹ÙŠØ¯Ø©', 'budget': 1200}
            }
        ]
        
        return jsonify({
            'success': True,
            'conflicts': conflicts,
            'total_conflicts': len(conflicts)
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª',
            'message': str(e)
        }), 500

@google_ads_sync_bp.route('/resolve-conflict', methods=['POST'])
@jwt_required()
async def resolve_conflict():
    """Ø­Ù„ ØªØ¹Ø§Ø±Ø¶ ÙŠØ¯ÙˆÙŠØ§Ù‹"""
    try:
        data = request.get_json() or {}
        
        conflict_id = data.get('conflict_id')
        resolution = data.get('resolution')  # 'local', 'remote', 'custom'
        custom_data = data.get('custom_data')
        
        if not conflict_id or not resolution:
            return jsonify({
                'success': False,
                'error': 'Ù…Ø¹Ø±Ù Ø§Ù„ØªØ¹Ø§Ø±Ø¶ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø­Ù„ Ù…Ø·Ù„ÙˆØ¨Ø§Ù†'
            }), 400
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù„
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ÙŠØªÙ… Ø¬Ù„Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶ ÙˆØ­Ù„Ù‡
        
        return jsonify({
            'success': True,
            'message': 'ØªÙ… Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶ Ø¨Ù†Ø¬Ø§Ø­',
            'conflict_id': conflict_id,
            'resolution_applied': resolution
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶: {e}")
        return jsonify({
            'success': False,
            'error': 'Ø®Ø·Ø£ ÙÙŠ Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶',
            'message': str(e)
        }), 500

@google_ads_sync_bp.route('/health', methods=['GET'])
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
    try:
        health_status = {
            'service': 'Google Ads Sync',
            'status': 'healthy',
            'version': '2.1.0',
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'services_status': SYNC_SERVICES_STATUS,
            'active_jobs': len(sync_engine.active_jobs),
            'total_syncs': sync_engine.sync_stats['total_syncs'],
            'success_rate': 0
        }
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
        total_syncs = sync_engine.sync_stats['total_syncs']
        if total_syncs > 0:
            success_rate = (sync_engine.sync_stats['successful_syncs'] / total_syncs) * 100
            health_status['success_rate'] = success_rate
        
        # ÙØ­Øµ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if not any(SYNC_SERVICES_STATUS.values()):
            health_status['status'] = 'degraded'
            health_status['warning'] = 'Ø¨Ø¹Ø¶ Ø§Ù„Ø®Ø¯Ù…Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­Ø©'
        
        return jsonify(health_status)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØµØ­Ø©: {e}")
        return jsonify({
            'service': 'Google Ads Sync',
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }), 500

# ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Blueprint
logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Google Ads Sync Blueprint - Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ù…ØªØ§Ø­Ø©: {SYNC_SERVICES_AVAILABLE}")
logger.info(f"ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª: {sum(SYNC_SERVICES_STATUS.values())}/7 Ù…ØªØ§Ø­Ø©")

# ØªØµØ¯ÙŠØ± Blueprint ÙˆØ§Ù„ÙƒÙ„Ø§Ø³Ø§Øª
__all__ = [
    'google_ads_sync_bp',
    'SyncEngine',
    'SyncConfig',
    'SyncResult',
    'SyncJob',
    'DataChange',
    'SyncType',
    'SyncStatus',
    'SyncPriority',
    'ConflictResolution',
    'DataEntity',
    'RateLimitManager',
    'ConflictResolver',
    'ChangeDetector'
]

