#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“Š Data Processor - Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø«
=========================================

Ù…Ø­Ø±Ùƒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø¯Ø¹Ù… Ù†Ø¸Ø§Ù… MCC Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ.
ÙŠØ¯Ø¹Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø§Ø¨Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ù…ØªØ²Ø§Ù…Ù†.

Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª MCC Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
- ØªØ­Ù„ÙŠÙ„ Ù…Ù‚Ø§Ø±Ù† Ù„Ù„Ø£Ø¯Ø§Ø¡
- ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ
- ØªÙ‚Ø§Ø±ÙŠØ± Ø´Ø§Ù…Ù„Ø© Ù…ÙˆØ­Ø¯Ø©
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ²Ø§Ù…Ù†Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©

Ø§Ù„Ù…Ø·ÙˆØ±: Google Ads AI Platform Team
Ø§Ù„ØªØ§Ø±ÙŠØ®: 2025-07-07
Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 2.0.0 (MCC Support)
"""

import logging
import asyncio
import json
import re
import os
from typing import Dict, Any, List, Optional, Tuple, Union, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from enum import Enum
import pandas as pd
import numpy as np
import hashlib
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics
from collections import defaultdict, Counter

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ­Ø¯Ø§Øª MCC
try:
    from ..mcc.mcc_manager import MCCManager, MCCAccount
    MCC_AVAILABLE = True
except ImportError:
    MCC_AVAILABLE = False
    MCCManager = None
    MCCAccount = None

from ..utils.logger import setup_logger

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„
logger = setup_logger(__name__)

class DataType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    CAMPAIGN = "campaign"
    AD_GROUP = "ad_group"
    KEYWORD = "keyword"
    AD = "ad"
    EXTENSION = "extension"
    AUDIENCE = "audience"
    PLACEMENT = "placement"
    DEMOGRAPHIC = "demographic"
    GEOGRAPHIC = "geographic"
    DEVICE = "device"
    TIME = "time"
    CONVERSION = "conversion"
    ATTRIBUTION = "attribution"

class ProcessingStatus(Enum):
    """Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class QualityLevel(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    CRITICAL = "critical"

@dataclass
class DataQualityMetrics:
    """
    ğŸ“Š Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    completeness: float = 0.0  # Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (0-1)
    accuracy: float = 0.0      # Ø¯Ù‚Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (0-1)
    consistency: float = 0.0   # Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (0-1)
    timeliness: float = 0.0    # Ø­Ø¯Ø§Ø«Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (0-1)
    validity: float = 0.0      # ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (0-1)
    uniqueness: float = 0.0    # ØªÙØ±Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (0-1)
    
    @property
    def overall_score(self) -> float:
        """Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        scores = [self.completeness, self.accuracy, self.consistency, 
                 self.timeliness, self.validity, self.uniqueness]
        return sum(scores) / len(scores)
    
    @property
    def quality_level(self) -> QualityLevel:
        """Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬ÙˆØ¯Ø©"""
        score = self.overall_score
        if score >= 0.9:
            return QualityLevel.EXCELLENT
        elif score >= 0.8:
            return QualityLevel.GOOD
        elif score >= 0.6:
            return QualityLevel.FAIR
        elif score >= 0.4:
            return QualityLevel.POOR
        else:
            return QualityLevel.CRITICAL
    
    def to_dict(self) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³"""
        return {
            'completeness': self.completeness,
            'accuracy': self.accuracy,
            'consistency': self.consistency,
            'timeliness': self.timeliness,
            'validity': self.validity,
            'uniqueness': self.uniqueness,
            'overall_score': self.overall_score,
            'quality_level': self.quality_level.value
        }

@dataclass
class ProcessingResult:
    """
    ğŸ“‹ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    """
    id: str
    data_type: DataType
    status: ProcessingStatus
    input_count: int = 0
    output_count: int = 0
    processed_count: int = 0
    error_count: int = 0
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    quality_metrics: Optional[DataQualityMetrics] = None
    processing_time: float = 0.0
    memory_usage: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def success_rate(self) -> float:
        """Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­"""
        if self.input_count == 0:
            return 0.0
        return (self.processed_count / self.input_count) * 100
    
    @property
    def error_rate(self) -> float:
        """Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        if self.input_count == 0:
            return 0.0
        return (self.error_count / self.input_count) * 100
    
    def to_dict(self) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³"""
        return {
            'id': self.id,
            'data_type': self.data_type.value,
            'status': self.status.value,
            'input_count': self.input_count,
            'output_count': self.output_count,
            'processed_count': self.processed_count,
            'error_count': self.error_count,
            'success_rate': self.success_rate,
            'error_rate': self.error_rate,
            'warnings': self.warnings,
            'errors': self.errors,
            'quality_metrics': self.quality_metrics.to_dict() if self.quality_metrics else None,
            'processing_time': self.processing_time,
            'memory_usage': self.memory_usage,
            'created_at': self.created_at.isoformat(),
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'metadata': self.metadata
        }

@dataclass
class MCCDataSummary:
    """
    ğŸ¢ Ù…Ù„Ø®Øµ Ø¨ÙŠØ§Ù†Ø§Øª MCC
    """
    mcc_id: str
    total_accounts: int = 0
    active_accounts: int = 0
    total_campaigns: int = 0
    total_ad_groups: int = 0
    total_keywords: int = 0
    total_ads: int = 0
    total_impressions: int = 0
    total_clicks: int = 0
    total_conversions: float = 0.0
    total_cost: float = 0.0
    average_ctr: float = 0.0
    average_cpc: float = 0.0
    average_conversion_rate: float = 0.0
    quality_score: float = 0.0
    data_freshness: datetime = field(default_factory=datetime.now)
    account_summaries: List[Dict[str, Any]] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³"""
        return asdict(self)

class DataProcessor:
    """
    ğŸ“Š Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø«
    
    Ù…Ø­Ø±Ùƒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø¯Ø¹Ù…:
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª MCC Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
    - ØªØ­Ù„ÙŠÙ„ Ù…Ù‚Ø§Ø±Ù† Ù„Ù„Ø£Ø¯Ø§Ø¡
    - ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ²Ø§Ù…Ù†Ø©
    """
    
    def __init__(self, mcc_manager: Optional['MCCManager'] = None, max_workers: int = 5):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            mcc_manager: Ù…Ø¯ÙŠØ± MCC
            max_workers: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        """
        self.mcc_manager = mcc_manager
        self.max_workers = max_workers
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.batch_size = int(os.getenv('DATA_PROCESSING_BATCH_SIZE', '1000'))
        self.timeout = int(os.getenv('DATA_PROCESSING_TIMEOUT', '300'))
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.cache = {}
        self.cache_ttl = timedelta(minutes=30)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_stats = {
            'total_processed': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'average_processing_time': 0.0,
            'total_data_volume': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        logger.info("ğŸ“Š ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø« Ù…Ø¹ Ø¯Ø¹Ù… MCC")
    
    async def process_data(
        self,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        data_type: DataType,
        processing_options: Optional[Dict[str, Any]] = None
    ) -> ProcessingResult:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            data: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§
            data_type: Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            processing_options: Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            
        Returns:
            ProcessingResult: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        start_time = datetime.now()
        result_id = f"proc_{int(start_time.timestamp())}_{data_type.value}"
        
        result = ProcessingResult(
            id=result_id,
            data_type=data_type,
            status=ProcessingStatus.PROCESSING,
            input_count=len(data) if data else 0
        )
        
        logger.info(f"ğŸ“Š Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {result.input_count} Ø¹Ù†ØµØ± Ù…Ù† Ù†ÙˆØ¹ {data_type.value}")
        
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ DataFrame Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if isinstance(data, list):
                df = pd.DataFrame(data)
            else:
                df = data.copy()
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            processed_df = await self._process_by_type(df, data_type, processing_options or {})
            
            # ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            quality_metrics = await self._assess_data_quality(processed_df, data_type)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†ØªÙŠØ¬Ø©
            result.status = ProcessingStatus.COMPLETED
            result.output_count = len(processed_df)
            result.processed_count = len(processed_df)
            result.quality_metrics = quality_metrics
            result.completed_at = datetime.now()
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            self._cache_result(result_id, processed_df)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.performance_stats['successful_operations'] += 1
            
            logger.info(f"âœ… ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­: {result.processed_count} Ø¹Ù†ØµØ±")
            
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.errors.append(str(e))
            self.performance_stats['failed_operations'] += 1
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        
        finally:
            # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            processing_time = (datetime.now() - start_time).total_seconds()
            result.processing_time = processing_time
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.performance_stats['total_processed'] += result.input_count
            self._update_average_processing_time(processing_time)
        
        return result
    
    async def process_mcc_accounts_data(
        self,
        accounts: Optional[List['MCCAccount']] = None,
        data_types: Optional[List[DataType]] = None,
        date_range: Optional[Tuple[datetime, datetime]] = None
    ) -> MCCDataSummary:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø§Ø¨Ø§Øª MCC
        
        Args:
            accounts: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©)
            data_types: Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            date_range: Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ§Ø±ÙŠØ®
            
        Returns:
            MCCDataSummary: Ù…Ù„Ø®Øµ Ø¨ÙŠØ§Ù†Ø§Øª MCC
        """
        if not MCC_AVAILABLE:
            raise ImportError("ÙˆØ­Ø¯Ø© MCC ØºÙŠØ± Ù…ØªØ§Ø­Ø©")
        
        if not self.mcc_manager:
            self.mcc_manager = MCCManager()
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
        if accounts is None:
            accounts = self.mcc_manager.get_client_accounts()
        
        if not accounts:
            logger.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø³Ø§Ø¨Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
            return MCCDataSummary(mcc_id="unknown")
        
        logger.info(f"ğŸ¢ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª {len(accounts)} Ø­Ø³Ø§Ø¨ MCC")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if data_types is None:
            data_types = [DataType.CAMPAIGN, DataType.AD_GROUP, DataType.KEYWORD, DataType.AD]
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ø­Ø³Ø§Ø¨
        account_summaries = []
        total_stats = {
            'campaigns': 0,
            'ad_groups': 0,
            'keywords': 0,
            'ads': 0,
            'impressions': 0,
            'clicks': 0,
            'conversions': 0.0,
            'cost': 0.0
        }
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ²Ø§Ù…Ù†Ø© Ù„Ù„Ø­Ø³Ø§Ø¨Ø§Øª
        tasks = []
        for account in accounts:
            task = self._process_account_data(account, data_types, date_range)
            tasks.append(task)
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù…
        account_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        active_accounts = 0
        for i, result in enumerate(account_results):
            if isinstance(result, Exception):
                logger.error(f"ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ {accounts[i].customer_id}: {result}")
                continue
            
            if result:
                account_summaries.append(result)
                active_accounts += 1
                
                # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                for key in total_stats:
                    total_stats[key] += result.get(key, 0)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
        average_ctr = (total_stats['clicks'] / total_stats['impressions'] * 100) if total_stats['impressions'] > 0 else 0
        average_cpc = (total_stats['cost'] / total_stats['clicks']) if total_stats['clicks'] > 0 else 0
        average_conversion_rate = (total_stats['conversions'] / total_stats['clicks'] * 100) if total_stats['clicks'] > 0 else 0
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ø®Øµ
        summary = MCCDataSummary(
            mcc_id=self.mcc_manager.mcc_customer_id if self.mcc_manager else "unknown",
            total_accounts=len(accounts),
            active_accounts=active_accounts,
            total_campaigns=total_stats['campaigns'],
            total_ad_groups=total_stats['ad_groups'],
            total_keywords=total_stats['keywords'],
            total_ads=total_stats['ads'],
            total_impressions=total_stats['impressions'],
            total_clicks=total_stats['clicks'],
            total_conversions=total_stats['conversions'],
            total_cost=total_stats['cost'],
            average_ctr=round(average_ctr, 2),
            average_cpc=round(average_cpc, 2),
            average_conversion_rate=round(average_conversion_rate, 2),
            account_summaries=account_summaries
        )
        
        logger.info(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ MCC: {active_accounts}/{len(accounts)} Ø­Ø³Ø§Ø¨ Ù†Ø´Ø·")
        
        return summary
    
    async def generate_comparative_report(
        self,
        accounts: List['MCCAccount'],
        metrics: List[str],
        date_range: Optional[Tuple[datetime, datetime]] = None
    ) -> Dict[str, Any]:
        """
        Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…Ù‚Ø§Ø±Ù† Ù„Ù„Ø­Ø³Ø§Ø¨Ø§Øª
        
        Args:
            accounts: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
            metrics: Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            date_range: Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ§Ø±ÙŠØ®
            
        Returns:
            Dict[str, Any]: Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†
        """
        logger.info(f"ğŸ“ˆ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…Ù‚Ø§Ø±Ù† Ù„Ù€ {len(accounts)} Ø­Ø³Ø§Ø¨")
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ø­Ø³Ø§Ø¨
        account_data = {}
        for account in accounts:
            try:
                data = await self._get_account_metrics(account, metrics, date_range)
                account_data[account.customer_id] = {
                    'name': account.name,
                    'data': data
                }
            except Exception as e:
                logger.error(f"ÙØ´Ù„ ÙÙŠ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ {account.customer_id}: {e}")
                continue
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        analysis = {
            'summary': self._analyze_account_performance(account_data, metrics),
            'rankings': self._rank_accounts(account_data, metrics),
            'trends': self._identify_trends(account_data, metrics),
            'recommendations': self._generate_recommendations(account_data, metrics),
            'detailed_data': account_data
        }
        
        return {
            'report_id': f"comparative_{int(datetime.now().timestamp())}",
            'generated_at': datetime.now().isoformat(),
            'accounts_count': len(accounts),
            'metrics': metrics,
            'date_range': {
                'start': date_range[0].isoformat() if date_range else None,
                'end': date_range[1].isoformat() if date_range else None
            },
            'analysis': analysis
        }
    
    async def _process_by_type(
        self,
        df: pd.DataFrame,
        data_type: DataType,
        options: Dict[str, Any]
    ) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹"""
        
        if data_type == DataType.CAMPAIGN:
            return await self._process_campaign_data(df, options)
        elif data_type == DataType.AD_GROUP:
            return await self._process_ad_group_data(df, options)
        elif data_type == DataType.KEYWORD:
            return await self._process_keyword_data(df, options)
        elif data_type == DataType.AD:
            return await self._process_ad_data(df, options)
        else:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø§Ù…Ø©
            return await self._process_generic_data(df, options)
    
    async def _process_campaign_data(self, df: pd.DataFrame, options: Dict[str, Any]) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù…Ù„Ø§Øª"""
        processed_df = df.copy()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        processed_df = processed_df.dropna(subset=['campaign_id', 'campaign_name'])
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹
        numeric_columns = ['impressions', 'clicks', 'cost', 'conversions']
        for col in numeric_columns:
            if col in processed_df.columns:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').fillna(0)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø´ØªÙ‚Ø©
        if 'impressions' in processed_df.columns and 'clicks' in processed_df.columns:
            processed_df['ctr'] = (processed_df['clicks'] / processed_df['impressions'] * 100).fillna(0)
        
        if 'cost' in processed_df.columns and 'clicks' in processed_df.columns:
            processed_df['cpc'] = (processed_df['cost'] / processed_df['clicks']).fillna(0)
        
        if 'conversions' in processed_df.columns and 'clicks' in processed_df.columns:
            processed_df['conversion_rate'] = (processed_df['conversions'] / processed_df['clicks'] * 100).fillna(0)
        
        return processed_df
    
    async def _process_ad_group_data(self, df: pd.DataFrame, options: Dict[str, Any]) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª"""
        processed_df = df.copy()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        processed_df = processed_df.dropna(subset=['ad_group_id', 'ad_group_name'])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù„Ø­Ù…Ù„Ø§Øª
        return await self._process_campaign_data(processed_df, options)
    
    async def _process_keyword_data(self, df: pd.DataFrame, options: Dict[str, Any]) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        processed_df = df.copy()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        processed_df = processed_df.dropna(subset=['keyword'])
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        processed_df['keyword_clean'] = processed_df['keyword'].str.lower().str.strip()
        
        # ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
        if 'match_type' not in processed_df.columns:
            processed_df['match_type'] = 'BROAD'
        
        return processed_df
    
    async def _process_ad_data(self, df: pd.DataFrame, options: Dict[str, Any]) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª"""
        processed_df = df.copy()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        processed_df = processed_df.dropna(subset=['ad_id'])
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†
        if 'headline' in processed_df.columns:
            processed_df['headline_length'] = processed_df['headline'].str.len()
        
        if 'description' in processed_df.columns:
            processed_df['description_length'] = processed_df['description'].str.len()
        
        return processed_df
    
    async def _process_generic_data(self, df: pd.DataFrame, options: Dict[str, Any]) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø§Ù…Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        processed_df = df.copy()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ©
        processed_df = processed_df.dropna(how='all')
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        for col in processed_df.columns:
            if processed_df[col].dtype == 'object':
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
                numeric_series = pd.to_numeric(processed_df[col], errors='coerce')
                if not numeric_series.isna().all():
                    processed_df[col] = numeric_series
        
        return processed_df
    
    async def _assess_data_quality(self, df: pd.DataFrame, data_type: DataType) -> DataQualityMetrics:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        metrics = DataQualityMetrics()
        
        if df.empty:
            return metrics
        
        # Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        total_cells = df.size
        non_null_cells = df.count().sum()
        metrics.completeness = non_null_cells / total_cells if total_cells > 0 else 0
        
        # Ø¯Ù‚Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØªØ­Ù‚Ù‚ Ø£Ø³Ø§Ø³ÙŠ)
        accuracy_score = 1.0
        
        # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø§Ù„Ø³Ø§Ù„Ø¨Ø© ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…ÙˆØ¬Ø¨Ø©
        positive_columns = ['impressions', 'clicks', 'cost', 'conversions']
        for col in positive_columns:
            if col in df.columns:
                negative_count = (df[col] < 0).sum()
                if negative_count > 0:
                    accuracy_score -= 0.1
        
        metrics.accuracy = max(0, accuracy_score)
        
        # Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        consistency_score = 1.0
        
        # ÙØ­Øµ Ø§ØªØ³Ø§Ù‚ CTR
        if all(col in df.columns for col in ['impressions', 'clicks', 'ctr']):
            calculated_ctr = (df['clicks'] / df['impressions'] * 100).fillna(0)
            ctr_diff = abs(df['ctr'] - calculated_ctr).mean()
            if ctr_diff > 1:  # ÙØ±Ù‚ Ø£ÙƒØ«Ø± Ù…Ù† 1%
                consistency_score -= 0.2
        
        metrics.consistency = max(0, consistency_score)
        
        # Ø­Ø¯Ø§Ø«Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if 'date' in df.columns:
            try:
                latest_date = pd.to_datetime(df['date']).max()
                days_old = (datetime.now() - latest_date).days
                metrics.timeliness = max(0, 1 - (days_old / 30))  # ØªÙ‚Ù„ Ø§Ù„Ø­Ø¯Ø§Ø«Ø© Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª
            except:
                metrics.timeliness = 0.5  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        else:
            metrics.timeliness = 0.8  # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø¯ÙŠØ«Ø©
        
        # ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        validity_score = 1.0
        
        # ÙØ­Øµ ØµØ­Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
        if 'customer_id' in df.columns:
            invalid_ids = df['customer_id'].astype(str).str.len() != 10
            if invalid_ids.any():
                validity_score -= 0.2
        
        metrics.validity = max(0, validity_score)
        
        # ØªÙØ±Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if 'id' in df.columns or f'{data_type.value}_id' in df.columns:
            id_col = 'id' if 'id' in df.columns else f'{data_type.value}_id'
            unique_ratio = df[id_col].nunique() / len(df)
            metrics.uniqueness = unique_ratio
        else:
            metrics.uniqueness = 0.9  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        
        return metrics
    
    async def _process_account_data(
        self,
        account: 'MCCAccount',
        data_types: List[DataType],
        date_range: Optional[Tuple[datetime, datetime]]
    ) -> Optional[Dict[str, Any]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø§Ø¨ ÙˆØ§Ø­Ø¯"""
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Ads API
            account_data = {
                'customer_id': account.customer_id,
                'name': account.name,
                'campaigns': 10,  # Ù…Ø­Ø§ÙƒØ§Ø©
                'ad_groups': 30,
                'keywords': 200,
                'ads': 50,
                'impressions': 10000,
                'clicks': 500,
                'conversions': 25.0,
                'cost': 1000.0
            }
            
            return account_data
            
        except Exception as e:
            logger.error(f"ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ {account.customer_id}: {e}")
            return None
    
    async def _get_account_metrics(
        self,
        account: 'MCCAccount',
        metrics: List[str],
        date_range: Optional[Tuple[datetime, datetime]]
    ) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø­Ø³Ø§Ø¨"""
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±ØºØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆÙ‡Ù…ÙŠØ©
        empty_data = {
            'impressions': 0,
            'clicks': 0,
            'cost': 0.0,
            'conversions': 0.0,
            'ctr': 0.0,
            'cpc': 0.0,
            'conversion_rate': 0.0
        }
        
        return {metric: empty_data.get(metric, 0) for metric in metrics}
    
    def _analyze_account_performance(self, account_data: Dict[str, Any], metrics: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª"""
        if not account_data:
            return {}
        
        analysis = {}
        
        for metric in metrics:
            values = [data['data'].get(metric, 0) for data in account_data.values()]
            
            if values:
                analysis[metric] = {
                    'average': statistics.mean(values),
                    'median': statistics.median(values),
                    'min': min(values),
                    'max': max(values),
                    'std_dev': statistics.stdev(values) if len(values) > 1 else 0
                }
        
        return analysis
    
    def _rank_accounts(self, account_data: Dict[str, Any], metrics: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """ØªØ±ØªÙŠØ¨ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        rankings = {}
        
        for metric in metrics:
            account_scores = []
            
            for customer_id, data in account_data.items():
                score = data['data'].get(metric, 0)
                account_scores.append({
                    'customer_id': customer_id,
                    'name': data['name'],
                    'score': score
                })
            
            # ØªØ±ØªÙŠØ¨ ØªÙ†Ø§Ø²Ù„ÙŠ
            account_scores.sort(key=lambda x: x['score'], reverse=True)
            rankings[metric] = account_scores
        
        return rankings
    
    def _identify_trends(self, account_data: Dict[str, Any], metrics: List[str]) -> Dict[str, Any]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        # ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
        trends = {}
        
        for metric in metrics:
            values = [data['data'].get(metric, 0) for data in account_data.values()]
            
            if values:
                avg_value = statistics.mean(values)
                trends[metric] = {
                    'trend': 'stable',  # Ù…Ø­Ø§ÙƒØ§Ø©
                    'average': avg_value,
                    'variance': statistics.variance(values) if len(values) > 1 else 0
                }
        
        return trends
    
    def _generate_recommendations(self, account_data: Dict[str, Any], metrics: List[str]) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª"""
        recommendations = []
        
        # ØªÙˆØµÙŠØ§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        if 'ctr' in metrics:
            ctr_values = [data['data'].get('ctr', 0) for data in account_data.values()]
            avg_ctr = statistics.mean(ctr_values) if ctr_values else 0
            
            if avg_ctr < 2:
                recommendations.append("Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø± Ù…Ù†Ø®ÙØ¶ - ÙŠÙÙ†ØµØ­ Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©")
        
        if 'conversion_rate' in metrics:
            conv_values = [data['data'].get('conversion_rate', 0) for data in account_data.values()]
            avg_conv = statistics.mean(conv_values) if conv_values else 0
            
            if avg_conv < 3:
                recommendations.append("Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù†Ø®ÙØ¶ - ÙŠÙÙ†ØµØ­ Ø¨ØªØ­Ø³ÙŠÙ† ØµÙØ­Ø§Øª Ø§Ù„Ù‡Ø¨ÙˆØ·")
        
        return recommendations
    
    def _cache_result(self, key: str, data: pd.DataFrame):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        self.cache[key] = {
            'data': data,
            'timestamp': datetime.now()
        }
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        self._cleanup_cache()
    
    def _cleanup_cache(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        current_time = datetime.now()
        expired_keys = []
        
        for key, value in self.cache.items():
            if current_time - value['timestamp'] > self.cache_ttl:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
    
    def _update_average_processing_time(self, processing_time: float):
        """ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        current_avg = self.performance_stats['average_processing_time']
        total_ops = self.performance_stats['successful_operations'] + self.performance_stats['failed_operations']
        
        if total_ops == 1:
            self.performance_stats['average_processing_time'] = processing_time
        else:
            new_avg = ((current_avg * (total_ops - 1)) + processing_time) / total_ops
            self.performance_stats['average_processing_time'] = new_avg
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        stats = self.performance_stats.copy()
        total_ops = stats['successful_operations'] + stats['failed_operations']
        stats['success_rate'] = (stats['successful_operations'] / total_ops * 100) if total_ops > 0 else 0
        stats['cache_hit_rate'] = (stats['cache_hits'] / (stats['cache_hits'] + stats['cache_misses']) * 100) if (stats['cache_hits'] + stats['cache_misses']) > 0 else 0
        return stats
    
    def reset_performance_stats(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        self.performance_stats = {
            'total_processed': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'average_processing_time': 0.0,
            'total_data_volume': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        logger.info("ğŸ“Š ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡")

# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹
def get_data_processor(mcc_manager: Optional['MCCManager'] = None) -> DataProcessor:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    return DataProcessor(mcc_manager=mcc_manager)

async def process_campaign_data(data: List[Dict[str, Any]]) -> ProcessingResult:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù…Ù„Ø§Øª"""
    processor = get_data_processor()
    return await processor.process_data(data, DataType.CAMPAIGN)

async def process_mcc_data(mcc_manager: 'MCCManager') -> MCCDataSummary:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª MCC"""
    processor = get_data_processor(mcc_manager)
    return await processor.process_mcc_accounts_data()

# ØªØµØ¯ÙŠØ± Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
__all__ = [
    'DataProcessor',
    'DataType',
    'ProcessingStatus',
    'QualityLevel',
    'DataQualityMetrics',
    'ProcessingResult',
    'MCCDataSummary',
    'get_data_processor',
    'process_campaign_data',
    'process_mcc_data'
]
