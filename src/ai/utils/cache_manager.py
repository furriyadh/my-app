#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ’¾ Cache Manager - Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
==============================================

Ù†Ø¸Ø§Ù… Ø´Ø§Ù…Ù„ Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙŠØ¯Ø¹Ù…:
- Ø£Ù†ÙˆØ§Ø¹ ØªØ®Ø²ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯Ø© (Memory, File, Redis)
- Ø§Ù†ØªÙ‡Ø§Ø¡ ØµÙ„Ø§Ø­ÙŠØ© Ø°ÙƒÙŠ (TTL)
- Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
- Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
- ØªÙ†Ø¸ÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ
- Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙˆØ§Ø³ØªØ¹Ø§Ø¯Ø©

Ø§Ù„Ù…Ø·ÙˆØ±: Google Ads AI Platform Team
Ø§Ù„ØªØ§Ø±ÙŠØ®: 2025-07-07
Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 1.0.0
"""

import logging
import asyncio
import json
import pickle
import gzip
import hashlib
import time
import os
from typing import Dict, Any, List, Optional, Union, Callable, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import threading
from abc import ABC, abstractmethod

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    redis = None

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
try:
    from .logger import setup_logger
    logger = setup_logger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)

class CacheType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
    MEMORY = "memory"     # Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
    FILE = "file"         # Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
    REDIS = "redis"       # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Redis
    HYBRID = "hybrid"     # Ù…Ø®ØªÙ„Ø·

class SerializationType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ³Ù„Ø³Ù„"""
    JSON = "json"
    PICKLE = "pickle"
    STRING = "string"

@dataclass
class CacheEntry:
    """
    ğŸ“¦ Ø¹Ù†ØµØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
    """
    key: str
    value: Any
    created_at: datetime = field(default_factory=datetime.now)
    expires_at: Optional[datetime] = None
    access_count: int = 0
    last_accessed: datetime = field(default_factory=datetime.now)
    size_bytes: int = 0
    compressed: bool = False
    encrypted: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def is_expired(self) -> bool:
        """ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©"""
        if self.expires_at is None:
            return False
        return datetime.now() > self.expires_at
    
    @property
    def age_seconds(self) -> float:
        """Ø¹Ù…Ø± Ø§Ù„Ø¹Ù†ØµØ± Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ"""
        return (datetime.now() - self.created_at).total_seconds()
    
    def touch(self):
        """ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø§Ù„ÙˆØµÙˆÙ„"""
        self.last_accessed = datetime.now()
        self.access_count += 1
    
    def to_dict(self) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³"""
        return {
            'key': self.key,
            'created_at': self.created_at.isoformat(),
            'expires_at': self.expires_at.isoformat() if self.expires_at else None,
            'access_count': self.access_count,
            'last_accessed': self.last_accessed.isoformat(),
            'size_bytes': self.size_bytes,
            'compressed': self.compressed,
            'encrypted': self.encrypted,
            'metadata': self.metadata,
            'is_expired': self.is_expired,
            'age_seconds': self.age_seconds
        }

class CacheBackend(ABC):
    """
    ğŸ—ï¸ ÙˆØ§Ø¬Ù‡Ø© Ø®Ù„ÙÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
    """
    
    @abstractmethod
    async def get(self, key: str) -> Optional[CacheEntry]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù†ØµØ±"""
        pass
    
    @abstractmethod
    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        **kwargs
    ) -> bool:
        """Ø­ÙØ¸ Ø¹Ù†ØµØ±"""
        pass
    
    @abstractmethod
    async def delete(self, key: str) -> bool:
        """Ø­Ø°Ù Ø¹Ù†ØµØ±"""
        pass
    
    @abstractmethod
    async def exists(self, key: str) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø¹Ù†ØµØ±"""
        pass
    
    @abstractmethod
    async def clear(self) -> bool:
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ±"""
        pass
    
    @abstractmethod
    async def keys(self, pattern: str = "*") -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­"""
        pass
    
    @abstractmethod
    async def size(self) -> int:
        """Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        pass

class MemoryCache(CacheBackend):
    """
    ğŸ§  ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    """
    
    def __init__(self, max_size: int = 1000):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        
        Args:
            max_size: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ±
        """
        self.max_size = max_size
        self.cache: Dict[str, CacheEntry] = {}
        self.lock = threading.RLock()
        
        logger.debug(f"ğŸ§  ØªÙ… ØªÙ‡ÙŠØ¦Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø­Ø¯ Ø£Ù‚ØµÙ‰: {max_size})")
    
    async def get(self, key: str) -> Optional[CacheEntry]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù†ØµØ±"""
        with self.lock:
            if key not in self.cache:
                return None
            
            entry = self.cache[key]
            
            # ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            if entry.is_expired:
                del self.cache[key]
                return None
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„
            entry.touch()
            return entry
    
    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        **kwargs
    ) -> bool:
        """Ø­ÙØ¸ Ø¹Ù†ØµØ±"""
        with self.lock:
            # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            expires_at = None
            if ttl is not None:
                expires_at = datetime.now() + timedelta(seconds=ttl)
            
            # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            try:
                size_bytes = len(pickle.dumps(value))
            except:
                size_bytes = len(str(value).encode('utf-8'))
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù†ØµØ±
            entry = CacheEntry(
                key=key,
                value=value,
                expires_at=expires_at,
                size_bytes=size_bytes,
                metadata=kwargs
            )
            
            # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£Ù‚ØµÙ‰
            if len(self.cache) >= self.max_size and key not in self.cache:
                await self._evict_lru()
            
            self.cache[key] = entry
            return True
    
    async def delete(self, key: str) -> bool:
        """Ø­Ø°Ù Ø¹Ù†ØµØ±"""
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                return True
            return False
    
    async def exists(self, key: str) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø¹Ù†ØµØ±"""
        entry = await self.get(key)
        return entry is not None
    
    async def clear(self) -> bool:
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ±"""
        with self.lock:
            self.cache.clear()
            return True
    
    async def keys(self, pattern: str = "*") -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­"""
        with self.lock:
            if pattern == "*":
                return list(self.cache.keys())
            
            # ØªØ·Ø¨ÙŠÙ‚ Ù†Ù…Ø· Ø¨Ø³ÙŠØ·
            import fnmatch
            return [key for key in self.cache.keys() if fnmatch.fnmatch(key, pattern)]
    
    async def size(self) -> int:
        """Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        with self.lock:
            return len(self.cache)
    
    async def _evict_lru(self):
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹"""
        if not self.cache:
            return
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
        lru_key = min(
            self.cache.keys(),
            key=lambda k: (self.cache[k].access_count, self.cache[k].last_accessed)
        )
        
        del self.cache[lru_key]
        logger.debug(f"ğŸ—‘ï¸ ØªÙ… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹: {lru_key}")

class FileCache(CacheBackend):
    """
    ğŸ“ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª
    """
    
    def __init__(
        self,
        cache_dir: str = "/tmp/cache",
        max_files: int = 10000,
        compress: bool = True
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª
        
        Args:
            cache_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            max_files: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª
            compress: Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª
        """
        self.cache_dir = cache_dir
        self.max_files = max_files
        self.compress = compress
        self.lock = threading.RLock()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        os.makedirs(cache_dir, exist_ok=True)
        
        logger.debug(f"ğŸ“ ØªÙ… ØªÙ‡ÙŠØ¦Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {cache_dir}")
    
    def _get_file_path(self, key: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù"""
        # ØªØ´ÙÙŠØ± Ø§Ù„Ù…ÙØªØ§Ø­ Ù„Ø¶Ù…Ø§Ù† Ø£Ø³Ù…Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø¢Ù…Ù†Ø©
        safe_key = hashlib.md5(key.encode()).hexdigest()
        extension = ".gz" if self.compress else ".cache"
        return os.path.join(self.cache_dir, f"{safe_key}{extension}")
    
    async def get(self, key: str) -> Optional[CacheEntry]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù†ØµØ±"""
        file_path = self._get_file_path(key)
        
        if not os.path.exists(file_path):
            return None
        
        try:
            with self.lock:
                # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
                if self.compress:
                    with gzip.open(file_path, 'rb') as f:
                        data = pickle.load(f)
                else:
                    with open(file_path, 'rb') as f:
                        data = pickle.load(f)
                
                entry = data['entry']
                
                # ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                if entry.is_expired:
                    os.remove(file_path)
                    return None
                
                # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„
                entry.touch()
                
                # Ø­ÙØ¸ Ø§Ù„ØªØ­Ø¯ÙŠØ«
                await self._save_entry(file_path, entry)
                
                return entry
                
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª {file_path}: {e}")
            # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªØ§Ù„Ù
            try:
                os.remove(file_path)
            except:
                pass
            return None
    
    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        **kwargs
    ) -> bool:
        """Ø­ÙØ¸ Ø¹Ù†ØµØ±"""
        try:
            # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            expires_at = None
            if ttl is not None:
                expires_at = datetime.now() + timedelta(seconds=ttl)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù†ØµØ±
            entry = CacheEntry(
                key=key,
                value=value,
                expires_at=expires_at,
                metadata=kwargs
            )
            
            # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ù„ÙØ§Øª
            await self._manage_file_limit()
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
            file_path = self._get_file_path(key)
            await self._save_entry(file_path, entry)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            return False
    
    async def _save_entry(self, file_path: str, entry: CacheEntry):
        """Ø­ÙØ¸ Ø§Ù„Ø¹Ù†ØµØ± ÙÙŠ Ù…Ù„Ù"""
        with self.lock:
            data = {
                'entry': entry,
                'saved_at': datetime.now()
            }
            
            if self.compress:
                with gzip.open(file_path, 'wb') as f:
                    pickle.dump(data, f)
            else:
                with open(file_path, 'wb') as f:
                    pickle.dump(data, f)
            
            # ØªØ­Ø¯ÙŠØ« Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
            entry.size_bytes = os.path.getsize(file_path)
    
    async def delete(self, key: str) -> bool:
        """Ø­Ø°Ù Ø¹Ù†ØµØ±"""
        file_path = self._get_file_path(key)
        
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                return True
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­Ø°Ù Ù…Ù„Ù Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
        
        return False
    
    async def exists(self, key: str) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø¹Ù†ØµØ±"""
        entry = await self.get(key)
        return entry is not None
    
    async def clear(self) -> bool:
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ±"""
        try:
            with self.lock:
                for filename in os.listdir(self.cache_dir):
                    if filename.endswith(('.cache', '.gz')):
                        file_path = os.path.join(self.cache_dir, filename)
                        os.remove(file_path)
            return True
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            return False
    
    async def keys(self, pattern: str = "*") -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­"""
        keys = []
        
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith(('.cache', '.gz')):
                    file_path = os.path.join(self.cache_dir, filename)
                    try:
                        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Ø§Ù„Ù…Ù„Ù
                        if self.compress:
                            with gzip.open(file_path, 'rb') as f:
                                data = pickle.load(f)
                        else:
                            with open(file_path, 'rb') as f:
                                data = pickle.load(f)
                        
                        entry = data['entry']
                        if not entry.is_expired:
                            keys.append(entry.key)
                        else:
                            # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                            os.remove(file_path)
                            
                    except:
                        # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªØ§Ù„Ù
                        try:
                            os.remove(file_path)
                        except:
                            pass
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØ§ØªÙŠØ­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ù…Ø·
        if pattern != "*":
            import fnmatch
            keys = [key for key in keys if fnmatch.fnmatch(key, pattern)]
        
        return keys
    
    async def size(self) -> int:
        """Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            count = 0
            for filename in os.listdir(self.cache_dir):
                if filename.endswith(('.cache', '.gz')):
                    count += 1
            return count
        except:
            return 0
    
    async def _manage_file_limit(self):
        """Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ù„ÙØ§Øª"""
        try:
            current_size = await self.size()
            if current_size >= self.max_files:
                # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù‚Ø¯Ù…
                files_info = []
                for filename in os.listdir(self.cache_dir):
                    if filename.endswith(('.cache', '.gz')):
                        file_path = os.path.join(self.cache_dir, filename)
                        mtime = os.path.getmtime(file_path)
                        files_info.append((file_path, mtime))
                
                # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ ÙˆÙ‚Øª Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
                files_info.sort(key=lambda x: x[1])
                
                # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù‚Ø¯Ù…
                files_to_delete = current_size - self.max_files + 1
                for i in range(min(files_to_delete, len(files_info))):
                    try:
                        os.remove(files_info[i][0])
                    except:
                        pass
                        
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø¥Ø¯Ø§Ø±Ø© Ø­Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª: {e}")

class RedisCache(CacheBackend):
    """
    ğŸ”´ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙÙŠ Redis
    """
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None,
        prefix: str = "cache:"
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙÙŠ Redis
        
        Args:
            host: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø®Ø§Ø¯Ù…
            port: Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ÙØ°
            db: Ø±Ù‚Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            password: ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±
            prefix: Ø¨Ø§Ø¯Ø¦Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­
        """
        if not REDIS_AVAILABLE:
            raise ImportError("Ù…ÙƒØªØ¨Ø© redis ØºÙŠØ± Ù…ØªØ§Ø­Ø©")
        
        self.prefix = prefix
        self.redis_client = redis.Redis(
            host=host,
            port=port,
            db=db,
            password=password,
            decode_responses=False
        )
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
        try:
            self.redis_client.ping()
            logger.debug(f"ğŸ”´ ØªÙ… ØªÙ‡ÙŠØ¦Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙÙŠ Redis: {host}:{port}")
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Redis: {e}")
            raise
    
    def _get_redis_key(self, key: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Redis"""
        return f"{self.prefix}{key}"
    
    async def get(self, key: str) -> Optional[CacheEntry]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù†ØµØ±"""
        try:
            redis_key = self._get_redis_key(key)
            data = self.redis_client.get(redis_key)
            
            if data is None:
                return None
            
            entry = pickle.loads(data)
            
            # ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© (Ø¥Ø¶Ø§ÙÙŠ)
            if entry.is_expired:
                await self.delete(key)
                return None
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„
            entry.touch()
            
            # Ø­ÙØ¸ Ø§Ù„ØªØ­Ø¯ÙŠØ«
            await self._save_entry(redis_key, entry)
            
            return entry
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† Redis: {e}")
            return None
    
    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        **kwargs
    ) -> bool:
        """Ø­ÙØ¸ Ø¹Ù†ØµØ±"""
        try:
            # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            expires_at = None
            if ttl is not None:
                expires_at = datetime.now() + timedelta(seconds=ttl)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù†ØµØ±
            entry = CacheEntry(
                key=key,
                value=value,
                expires_at=expires_at,
                metadata=kwargs
            )
            
            # Ø­ÙØ¸ ÙÙŠ Redis
            redis_key = self._get_redis_key(key)
            await self._save_entry(redis_key, entry, ttl)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ ÙÙŠ Redis: {e}")
            return False
    
    async def _save_entry(
        self,
        redis_key: str,
        entry: CacheEntry,
        ttl: Optional[int] = None
    ):
        """Ø­ÙØ¸ Ø§Ù„Ø¹Ù†ØµØ± ÙÙŠ Redis"""
        data = pickle.dumps(entry)
        entry.size_bytes = len(data)
        
        if ttl is not None:
            self.redis_client.setex(redis_key, ttl, data)
        else:
            self.redis_client.set(redis_key, data)
    
    async def delete(self, key: str) -> bool:
        """Ø­Ø°Ù Ø¹Ù†ØµØ±"""
        try:
            redis_key = self._get_redis_key(key)
            result = self.redis_client.delete(redis_key)
            return result > 0
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­Ø°Ù Ù…Ù† Redis: {e}")
            return False
    
    async def exists(self, key: str) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø¹Ù†ØµØ±"""
        try:
            redis_key = self._get_redis_key(key)
            return self.redis_client.exists(redis_key) > 0
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¹Ù†ØµØ± ÙÙŠ Redis: {e}")
            return False
    
    async def clear(self) -> bool:
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ±"""
        try:
            pattern = f"{self.prefix}*"
            keys = self.redis_client.keys(pattern)
            if keys:
                self.redis_client.delete(*keys)
            return True
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø³Ø­ Redis: {e}")
            return False
    
    async def keys(self, pattern: str = "*") -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­"""
        try:
            redis_pattern = f"{self.prefix}{pattern}"
            redis_keys = self.redis_client.keys(redis_pattern)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø©
            keys = []
            for redis_key in redis_keys:
                if isinstance(redis_key, bytes):
                    redis_key = redis_key.decode('utf-8')
                key = redis_key[len(self.prefix):]
                keys.append(key)
            
            return keys
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØ§ØªÙŠØ­ Redis: {e}")
            return []
    
    async def size(self) -> int:
        """Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            pattern = f"{self.prefix}*"
            keys = self.redis_client.keys(pattern)
            return len(keys)
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Redis: {e}")
            return 0

class CacheManager:
    """
    ğŸ’¾ Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    
    ÙŠÙˆÙØ± ÙˆØ§Ø¬Ù‡Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù…Ø¹ Ø¯Ø¹Ù…:
    - Ø£Ù†ÙˆØ§Ø¹ ØªØ®Ø²ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯Ø©
    - Ø¶ØºØ· ÙˆØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    - ØªÙ†Ø¸ÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ
    """
    
    def __init__(
        self,
        backend: CacheBackend,
        default_ttl: int = 3600,
        enable_compression: bool = False,
        enable_encryption: bool = False
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        
        Args:
            backend: Ø®Ù„ÙÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            default_ttl: Ù…Ø¯Ø© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© (Ø«Ø§Ù†ÙŠØ©)
            enable_compression: ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø¶ØºØ·
            enable_encryption: ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªØ´ÙÙŠØ±
        """
        self.backend = backend
        self.default_ttl = default_ttl
        self.enable_compression = enable_compression
        self.enable_encryption = enable_encryption
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0,
            'errors': 0,
            'total_size_bytes': 0,
            'start_time': datetime.now()
        }
        
        # Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        self.cleanup_task = None
        self.cleanup_interval = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
        
        logger.info("ğŸ’¾ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
    
    async def get(self, key: str, default=None) -> Any:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        
        Args:
            key: Ø§Ù„Ù…ÙØªØ§Ø­
            default: Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            
        Returns:
            Any: Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© Ø£Ùˆ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        """
        try:
            entry = await self.backend.get(key)
            
            if entry is not None:
                self.stats['hits'] += 1
                value = entry.value
                
                # ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                if entry.compressed and self.enable_compression:
                    value = self._decompress_data(value)
                
                # ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                if entry.encrypted and self.enable_encryption:
                    value = self._decrypt_data(value)
                
                return value
            else:
                self.stats['misses'] += 1
                return default
                
        except Exception as e:
            self.stats['errors'] += 1
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ {key}: {e}")
            return default
    
    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        compress: Optional[bool] = None,
        encrypt: Optional[bool] = None,
        **kwargs
    ) -> bool:
        """
        Ø­ÙØ¸ Ù‚ÙŠÙ…Ø© ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        
        Args:
            key: Ø§Ù„Ù…ÙØªØ§Ø­
            value: Ø§Ù„Ù‚ÙŠÙ…Ø©
            ttl: Ù…Ø¯Ø© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© (Ø«Ø§Ù†ÙŠØ©)
            compress: Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            encrypt: ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            **kwargs: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
            
        Returns:
            bool: Ù†Ø¬Ø­ Ø§Ù„Ø­ÙØ¸ Ø£Ù… Ù„Ø§
        """
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            if ttl is None:
                ttl = self.default_ttl
            
            if compress is None:
                compress = self.enable_compression
            
            if encrypt is None:
                encrypt = self.enable_encryption
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            processed_value = value
            
            # Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if compress:
                processed_value = self._compress_data(processed_value)
                kwargs['compressed'] = True
            
            # ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if encrypt:
                processed_value = self._encrypt_data(processed_value)
                kwargs['encrypted'] = True
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            success = await self.backend.set(key, processed_value, ttl, **kwargs)
            
            if success:
                self.stats['sets'] += 1
            else:
                self.stats['errors'] += 1
            
            return success
            
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ {key}: {e}")
            return False
    
    async def delete(self, key: str) -> bool:
        """
        Ø­Ø°Ù Ø¹Ù†ØµØ± Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        
        Args:
            key: Ø§Ù„Ù…ÙØªØ§Ø­
            
        Returns:
            bool: Ù†Ø¬Ø­ Ø§Ù„Ø­Ø°Ù Ø£Ù… Ù„Ø§
        """
        try:
            success = await self.backend.delete(key)
            
            if success:
                self.stats['deletes'] += 1
            
            return success
            
        except Exception as e:
            self.stats['errors'] += 1
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­Ø°Ù {key}: {e}")
            return False
    
    async def exists(self, key: str) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ù…ÙØªØ§Ø­"""
        try:
            return await self.backend.exists(key)
        except Exception as e:
            self.stats['errors'] += 1
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ÙØ­Øµ ÙˆØ¬ÙˆØ¯ {key}: {e}")
            return False
    
    async def clear(self) -> bool:
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ±"""
        try:
            success = await self.backend.clear()
            
            if success:
                # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                self.stats.update({
                    'hits': 0,
                    'misses': 0,
                    'sets': 0,
                    'deletes': 0,
                    'errors': 0,
                    'total_size_bytes': 0
                })
            
            return success
            
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            return False
    
    async def keys(self, pattern: str = "*") -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­"""
        try:
            return await self.backend.keys(pattern)
        except Exception as e:
            self.stats['errors'] += 1
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­: {e}")
            return []
    
    async def size(self) -> int:
        """Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            return await self.backend.size()
        except Exception as e:
            self.stats['errors'] += 1
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¬Ù…: {e}")
            return 0
    
    def _compress_data(self, data: Any) -> bytes:
        """Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            serialized = pickle.dumps(data)
            compressed = gzip.compress(serialized)
            return compressed
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return pickle.dumps(data)
    
    def _decompress_data(self, data: bytes) -> Any:
        """ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            decompressed = gzip.decompress(data)
            return pickle.loads(decompressed)
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return pickle.loads(data)
    
    def _encrypt_data(self, data: Any) -> bytes:
        """ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØªÙ†ÙÙŠØ° Ø¨Ø³ÙŠØ·)"""
        # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ø§ ØªÙ†ÙÙŠØ° Ø¨Ø³ÙŠØ· Ù„Ù„ØªÙˆØ¶ÙŠØ­
        # ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø© ØªØ´ÙÙŠØ± Ù‚ÙˆÙŠØ© Ù…Ø«Ù„ cryptography
        try:
            import base64
            serialized = pickle.dumps(data)
            encoded = base64.b64encode(serialized)
            return encoded
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return pickle.dumps(data)
    
    def _decrypt_data(self, data: bytes) -> Any:
        """ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØªÙ†ÙÙŠØ° Ø¨Ø³ÙŠØ·)"""
        try:
            import base64
            decoded = base64.b64decode(data)
            return pickle.loads(decoded)
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return pickle.loads(data)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        total_requests = self.stats['hits'] + self.stats['misses']
        hit_rate = (self.stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        
        uptime = (datetime.now() - self.stats['start_time']).total_seconds()
        
        return {
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'sets': self.stats['sets'],
            'deletes': self.stats['deletes'],
            'errors': self.stats['errors'],
            'total_requests': total_requests,
            'hit_rate_percentage': round(hit_rate, 2),
            'error_rate_percentage': round(
                (self.stats['errors'] / max(total_requests, 1) * 100), 2
            ),
            'uptime_seconds': uptime,
            'requests_per_second': round(total_requests / max(uptime, 1), 2)
        }
    
    def reset_statistics(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        self.stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0,
            'errors': 0,
            'total_size_bytes': 0,
            'start_time': datetime.now()
        }
        logger.info("ğŸ“Š ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
    
    async def start_cleanup_task(self):
        """Ø¨Ø¯Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        if self.cleanup_task is None:
            self.cleanup_task = asyncio.create_task(self._cleanup_loop())
            logger.info("ğŸ§¹ ØªÙ… Ø¨Ø¯Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ")
    
    async def stop_cleanup_task(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass
            self.cleanup_task = None
            logger.info("ğŸ›‘ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ")
    
    async def _cleanup_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)
                await self._cleanup_expired()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {e}")
    
    async def _cleanup_expired(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©"""
        try:
            keys = await self.keys()
            expired_count = 0
            
            for key in keys:
                entry = await self.backend.get(key)
                if entry and entry.is_expired:
                    await self.delete(key)
                    expired_count += 1
            
            if expired_count > 0:
                logger.debug(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ {expired_count} Ø¹Ù†ØµØ± Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©: {e}")

# Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø¹Ø§Ù…
_global_cache_manager = None

def get_cache_manager(
    cache_type: CacheType = CacheType.MEMORY,
    **kwargs
) -> CacheManager:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø¹Ø§Ù…
    
    Args:
        cache_type: Ù†ÙˆØ¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        **kwargs: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        
    Returns:
        CacheManager: Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
    """
    global _global_cache_manager
    
    if _global_cache_manager is None:
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
        if cache_type == CacheType.MEMORY:
            backend = MemoryCache(max_size=kwargs.get('max_size', 1000))
        elif cache_type == CacheType.FILE:
            backend = FileCache(
                cache_dir=kwargs.get('cache_dir', '/tmp/cache'),
                max_files=kwargs.get('max_files', 10000),
                compress=kwargs.get('compress', True)
            )
        elif cache_type == CacheType.REDIS:
            backend = RedisCache(
                host=kwargs.get('host', 'localhost'),
                port=kwargs.get('port', 6379),
                db=kwargs.get('db', 0),
                password=kwargs.get('password'),
                prefix=kwargs.get('prefix', 'cache:')
            )
        else:
            # Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ø°Ø§ÙƒØ±Ø©
            backend = MemoryCache()
        
        _global_cache_manager = CacheManager(
            backend=backend,
            default_ttl=kwargs.get('default_ttl', 3600),
            enable_compression=kwargs.get('enable_compression', False),
            enable_encryption=kwargs.get('enable_encryption', False)
        )
    
    return _global_cache_manager

# ØªØµØ¯ÙŠØ± Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
__all__ = [
    'CacheManager',
    'CacheType',
    'CacheEntry',
    'CacheBackend',
    'MemoryCache',
    'FileCache',
    'RedisCache',
    'SerializationType',
    'get_cache_manager'
]

